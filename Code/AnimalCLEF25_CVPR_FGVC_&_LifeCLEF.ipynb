{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "02WxIoAKL9LZ",
        "qdfcgZ_zMFfE",
        "XdPo24hwTS_3",
        "yL-c_ncVMF9S",
        "CqIhBIAhxIux",
        "0IiN-rqukMOb",
        "YGPavhyEDKu-",
        "Z4wbQawJHiBz",
        "sTgFOpoHcj6f",
        "IxlObTO_OP-j",
        "0Xi7kMRKpgHv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>AnimalCLEF25 @ CVPR-FGVC & LifeCLEF<h1>"
      ],
      "metadata": {
        "id": "ihmV5Vb9CDZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ImageNet Benchmark:\n",
        "\n",
        "https://paperswithcode.com/sota/image-classification-on-imagenet?p=deepvit-towards-deeper-vision-transformer"
      ],
      "metadata": {
        "id": "GsaZs3n599Cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swin Checkpoints:\n",
        "\n",
        "https://github.com/microsoft/Swin-Transformer/blob/main/MODELHUB.md#simmim-pretrained-swin-v2-models"
      ],
      "metadata": {
        "id": "CTTUhrzTGgk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Analysis**"
      ],
      "metadata": {
        "id": "02WxIoAKL9LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AnimalCLEF**"
      ],
      "metadata": {
        "id": "jd3ZTvh9CQ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"metadata.csv\")"
      ],
      "metadata": {
        "id": "OWyTZ8xuE1jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDMgyjKyFNbp",
        "outputId": "b0093936-b945-4dd4-aa69-f6bf7eb5bff4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15209 entries, 0 to 15208\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     15209 non-null  int64 \n",
            " 1   identity     13074 non-null  object\n",
            " 2   path         15209 non-null  object\n",
            " 3   date         11302 non-null  object\n",
            " 4   orientation  14506 non-null  object\n",
            " 5   species      13821 non-null  object\n",
            " 6   split        15209 non-null  object\n",
            " 7   dataset      15209 non-null  object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 950.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database_df = df[df['split'] == 'database']\n",
        "query_df = df[df['split'] == 'query']"
      ],
      "metadata": {
        "id": "a7te02eqE4Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCnxxgdUFVzJ",
        "outputId": "2eea053f-bc6d-436b-c65c-93e5c1797005",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 13074 entries, 0 to 14708\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     13074 non-null  int64 \n",
            " 1   identity     13074 non-null  object\n",
            " 2   path         13074 non-null  object\n",
            " 3   date         10113 non-null  object\n",
            " 4   orientation  12871 non-null  object\n",
            " 5   species      11686 non-null  object\n",
            " 6   split        13074 non-null  object\n",
            " 7   dataset      13074 non-null  object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 919.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database_df[\"identity\"].info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc4cbQUBIkze",
        "outputId": "5efe949e-1cec-4871-c776-2da11612fc73",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Index: 13074 entries, 0 to 14708\n",
            "Series name: identity\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "13074 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 204.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database_df = pd.read_csv('database_metadata.csv')\n",
        "\n",
        "identity_counts = database_df['identity'].value_counts()\n",
        "single_occurrence_identities = identity_counts[identity_counts == 1].index\n",
        "\n",
        "print(single_occurrence_identities)\n",
        "print()\n",
        "print(len(single_occurrence_identities))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ugLInsJImm",
        "outputId": "990737f2-472d-45de-edb0-a88836e37618",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['SalamanderID2025_250', 'SalamanderID2025_216', 'SalamanderID2025_217',\n",
            "       'SalamanderID2025_221', 'SalamanderID2025_222', 'SalamanderID2025_223',\n",
            "       'SalamanderID2025_224', 'SalamanderID2025_225', 'SalamanderID2025_229',\n",
            "       'SeaTurtleID2022_t103',\n",
            "       ...\n",
            "       'SalamanderID2025_126', 'SalamanderID2025_125', 'SalamanderID2025_124',\n",
            "       'SalamanderID2025_167', 'SalamanderID2025_166', 'SalamanderID2025_163',\n",
            "       'SalamanderID2025_159', 'SalamanderID2025_158', 'SalamanderID2025_154',\n",
            "       'SalamanderID2025_153'],\n",
            "      dtype='object', name='identity', length=317)\n",
            "\n",
            "317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppo-9BUhFZCT",
        "outputId": "842f4268-fac0-4c86-a7d8-077736337719",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2135 entries, 3 to 15208\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     2135 non-null   int64 \n",
            " 1   identity     0 non-null      object\n",
            " 2   path         2135 non-null   object\n",
            " 3   date         1189 non-null   object\n",
            " 4   orientation  1635 non-null   object\n",
            " 5   species      2135 non-null   object\n",
            " 6   split        2135 non-null   object\n",
            " 7   dataset      2135 non-null   object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 150.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database_df.to_csv(\"database_metadata.csv\", index=False)\n",
        "query_df.to_csv(\"query_metadata.csv\", index=False)"
      ],
      "metadata": {
        "id": "-axklKIZE7DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load the database_metadata.csv\n",
        "database_df = pd.read_csv('database_metadata.csv')\n",
        "\n",
        "# 2. Find identities that appear only once\n",
        "identity_counts = database_df['identity'].value_counts()\n",
        "single_occurrence_identities = identity_counts[identity_counts == 1].index\n",
        "\n",
        "# 3. Split the data:\n",
        "# - Images with identities appearing only once\n",
        "single_occurrence_df = database_df[database_df['identity'].isin(single_occurrence_identities)]\n",
        "print(\"single_occurrence_df:\")\n",
        "single_occurrence_df.info()\n",
        "print()\n",
        "\n",
        "# - Images with identities appearing at least twice\n",
        "normal_df = database_df[~database_df['identity'].isin(single_occurrence_identities)]\n",
        "print(\"normal_df:\")\n",
        "normal_df.info()\n",
        "print()\n",
        "\n",
        "# 4. Perform stratified train/validation split on normal identities\n",
        "train_normal_df, test_normal_df = train_test_split(\n",
        "    normal_df,\n",
        "    test_size=0.2,\n",
        "    stratify=normal_df['identity'],\n",
        "    random_state=42\n",
        ")\n",
        "print(\"train_normal.df:\")\n",
        "train_normal_df.info()\n",
        "print()\n",
        "print(\"test_normal.df:\")\n",
        "test_normal_df.info()\n",
        "print()\n",
        "\n",
        "# 5. Combine all single-occurrence identities + 80% of normal identities into the training set\n",
        "train_df = pd.concat([train_normal_df, single_occurrence_df]).reset_index(drop=True)\n",
        "\n",
        "# 6. Save the CSV files\n",
        "train_df.to_csv('train_database_metadata.csv', index=False)\n",
        "test_normal_df.to_csv('test_database_metadata.csv', index=False)\n",
        "\n",
        "print(f\"Train set: {len(train_df)} images\")\n",
        "print(f\"Test set: {len(test_normal_df)} images\")\n",
        "split = len(test_normal_df)/len(train_df)\n",
        "print(f\"Split: {split}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44v_Fx-vG3kk",
        "outputId": "a541d45a-5a95-4674-91ee-2eab8944252d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "single_occurrence_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 317 entries, 678 to 6309\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     317 non-null    int64 \n",
            " 1   identity     317 non-null    object\n",
            " 2   path         317 non-null    object\n",
            " 3   date         308 non-null    object\n",
            " 4   orientation  317 non-null    object\n",
            " 5   species      7 non-null      object\n",
            " 6   split        317 non-null    object\n",
            " 7   dataset      317 non-null    object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 22.3+ KB\n",
            "\n",
            "normal_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 12757 entries, 0 to 13073\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     12757 non-null  int64 \n",
            " 1   identity     12757 non-null  object\n",
            " 2   path         12757 non-null  object\n",
            " 3   date         9805 non-null   object\n",
            " 4   orientation  12554 non-null  object\n",
            " 5   species      11679 non-null  object\n",
            " 6   split        12757 non-null  object\n",
            " 7   dataset      12757 non-null  object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 897.0+ KB\n",
            "\n",
            "train_normal.df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 10205 entries, 12224 to 3256\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     10205 non-null  int64 \n",
            " 1   identity     10205 non-null  object\n",
            " 2   path         10205 non-null  object\n",
            " 3   date         7847 non-null   object\n",
            " 4   orientation  10044 non-null  object\n",
            " 5   species      9327 non-null   object\n",
            " 6   split        10205 non-null  object\n",
            " 7   dataset      10205 non-null  object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 717.5+ KB\n",
            "\n",
            "test_normal.df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2552 entries, 5070 to 8050\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_id     2552 non-null   int64 \n",
            " 1   identity     2552 non-null   object\n",
            " 2   path         2552 non-null   object\n",
            " 3   date         1958 non-null   object\n",
            " 4   orientation  2510 non-null   object\n",
            " 5   species      2352 non-null   object\n",
            " 6   split        2552 non-null   object\n",
            " 7   dataset      2552 non-null   object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 179.4+ KB\n",
            "\n",
            "Train set: 10522 images\n",
            "Test set: 2552 images\n",
            "Split: 0.24253944117088005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train_database_metadata.csv')\n",
        "test_df = pd.read_csv('test_database_metadata.csv')\n",
        "\n",
        "train_identities = set(train_df['identity'].unique())\n",
        "test_identities = set(test_df['identity'].unique())\n",
        "\n",
        "missing_identities = test_identities - train_identities\n",
        "\n",
        "if len(missing_identities) == 0:\n",
        "    print(\"✅ All identities in the test set are present in the train set.\")\n",
        "else:\n",
        "    print(f\"❌ {len(missing_identities)} identities in the test set are missing from the train set!\")\n",
        "    print(\"Missing identities:\")\n",
        "    print(missing_identities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZMy9iF3S7u9",
        "outputId": "ce5b2185-086e-4290-8e8e-4937f74e93cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All identities in the test set are present in the train set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WildlifeReID-10k**"
      ],
      "metadata": {
        "id": "lFeXnr6CPEQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = \"/content/metadata.csv\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"Split values:\", df[\"split\"].unique())\n",
        "\n",
        "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
        "test_df = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
        "\n",
        "train_df.to_csv(\"train_metadata.csv\", index=False)\n",
        "test_df.to_csv(\"test_metadata.csv\", index=False)\n",
        "\n",
        "print(f\"✅ Train set: {len(train_df)} samples\")\n",
        "print(f\"✅ Test set: {len(test_df)} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAUi7w-nPHFq",
        "outputId": "e0bbfe46-8b80-4d70-8d5d-e559760640ef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a4784bcf0197>:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split values: ['train' 'test']\n",
            "✅ Train set: 109927 samples\n",
            "✅ Test set: 30561 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train_metadata.csv\")\n",
        "test_df = pd.read_csv(\"test_metadata.csv\")\n",
        "\n",
        "train_identities = set(train_df[\"identity\"].unique())\n",
        "test_identities = set(test_df[\"identity\"].unique())\n",
        "\n",
        "unseen_identities = test_identities - train_identities\n",
        "\n",
        "print(f\"Number of unseen identities in test: {len(unseen_identities)}\")\n",
        "print(\"Unseen identities:\")\n",
        "for identity in sorted(unseen_identities):\n",
        "    print(identity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_v5_AEvBjPx",
        "outputId": "6d6dc67f-443b-4ce1-fc1e-ed8c410211fd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Număr de identități necunoscute în test: 523\n",
            "Identitățile necunoscute:\n",
            "AAUZebraFish_2\n",
            "ATRW_118\n",
            "ATRW_12\n",
            "ATRW_181\n",
            "ATRW_226\n",
            "ATRW_262\n",
            "ATRW_270\n",
            "ATRW_49\n",
            "ATRW_73\n",
            "AerialCattle2017_10\n",
            "AerialCattle2017_5\n",
            "AmvrakikosTurtles_36\n",
            "AmvrakikosTurtles_40\n",
            "AmvrakikosTurtles_42\n",
            "BelugaID_whale022\n",
            "BelugaID_whale035\n",
            "BelugaID_whale038\n",
            "BelugaID_whale067\n",
            "BelugaID_whale088\n",
            "BelugaID_whale089\n",
            "BelugaID_whale180\n",
            "BelugaID_whale224\n",
            "BelugaID_whale298\n",
            "BelugaID_whale306\n",
            "BelugaID_whale339\n",
            "BelugaID_whale342\n",
            "BelugaID_whale346\n",
            "BelugaID_whale350\n",
            "BelugaID_whale371\n",
            "BelugaID_whale381\n",
            "BelugaID_whale398\n",
            "BelugaID_whale410\n",
            "BelugaID_whale437\n",
            "BelugaID_whale450\n",
            "BelugaID_whale456\n",
            "BelugaID_whale474\n",
            "BelugaID_whale483\n",
            "BelugaID_whale486\n",
            "BelugaID_whale490\n",
            "BelugaID_whale531\n",
            "BelugaID_whale536\n",
            "BelugaID_whale543\n",
            "BelugaID_whale578\n",
            "BelugaID_whale607\n",
            "BelugaID_whale620\n",
            "BelugaID_whale629\n",
            "BelugaID_whale668\n",
            "BelugaID_whale669\n",
            "BelugaID_whale698\n",
            "BelugaID_whale716\n",
            "BelugaID_whale717\n",
            "BelugaID_whale721\n",
            "BelugaID_whale731\n",
            "BelugaID_whale733\n",
            "BelugaID_whale779\n",
            "BirdIndividualID_01101787FB\n",
            "BirdIndividualID_0700EE27D0\n",
            "BirdIndividualID_0700EE27D7\n",
            "CTai_Duna\n",
            "CTai_Kuba\n",
            "CTai_Wala\n",
            "CZoo_Patrick\n",
            "CZoo_Tai\n",
            "CatIndividualImages_172\n",
            "CatIndividualImages_197\n",
            "CatIndividualImages_216\n",
            "CatIndividualImages_217\n",
            "CatIndividualImages_225\n",
            "CatIndividualImages_231\n",
            "CatIndividualImages_234\n",
            "CatIndividualImages_248\n",
            "CatIndividualImages_259\n",
            "CatIndividualImages_277\n",
            "CatIndividualImages_30\n",
            "CatIndividualImages_300\n",
            "CatIndividualImages_315\n",
            "CatIndividualImages_368\n",
            "CatIndividualImages_370\n",
            "CatIndividualImages_398\n",
            "CatIndividualImages_455\n",
            "CatIndividualImages_480\n",
            "CatIndividualImages_491\n",
            "CatIndividualImages_502\n",
            "CatIndividualImages_76\n",
            "CatIndividualImages_78\n",
            "CatIndividualImages_99\n",
            "Chicks4FreeID_1\n",
            "Chicks4FreeID_33\n",
            "CowDataset_12\n",
            "Cows2021_112\n",
            "Cows2021_121\n",
            "Cows2021_142\n",
            "Cows2021_149\n",
            "Cows2021_170\n",
            "Cows2021_183\n",
            "Cows2021_45\n",
            "Cows2021_59\n",
            "Cows2021_93\n",
            "DogFaceNet_1001\n",
            "DogFaceNet_1011\n",
            "DogFaceNet_102\n",
            "DogFaceNet_1022\n",
            "DogFaceNet_1038\n",
            "DogFaceNet_1058\n",
            "DogFaceNet_1061\n",
            "DogFaceNet_1086\n",
            "DogFaceNet_1092\n",
            "DogFaceNet_1113\n",
            "DogFaceNet_1147\n",
            "DogFaceNet_1213\n",
            "DogFaceNet_1218\n",
            "DogFaceNet_1256\n",
            "DogFaceNet_1269\n",
            "DogFaceNet_1319\n",
            "DogFaceNet_1337\n",
            "DogFaceNet_1345\n",
            "DogFaceNet_1386\n",
            "DogFaceNet_139\n",
            "DogFaceNet_1411\n",
            "DogFaceNet_160\n",
            "DogFaceNet_2\n",
            "DogFaceNet_200\n",
            "DogFaceNet_202\n",
            "DogFaceNet_204\n",
            "DogFaceNet_278\n",
            "DogFaceNet_280\n",
            "DogFaceNet_294\n",
            "DogFaceNet_302\n",
            "DogFaceNet_322\n",
            "DogFaceNet_340\n",
            "DogFaceNet_359\n",
            "DogFaceNet_366\n",
            "DogFaceNet_385\n",
            "DogFaceNet_404\n",
            "DogFaceNet_418\n",
            "DogFaceNet_444\n",
            "DogFaceNet_475\n",
            "DogFaceNet_488\n",
            "DogFaceNet_523\n",
            "DogFaceNet_532\n",
            "DogFaceNet_549\n",
            "DogFaceNet_558\n",
            "DogFaceNet_56\n",
            "DogFaceNet_597\n",
            "DogFaceNet_612\n",
            "DogFaceNet_618\n",
            "DogFaceNet_66\n",
            "DogFaceNet_675\n",
            "DogFaceNet_679\n",
            "DogFaceNet_762\n",
            "DogFaceNet_773\n",
            "DogFaceNet_776\n",
            "DogFaceNet_783\n",
            "DogFaceNet_801\n",
            "DogFaceNet_837\n",
            "DogFaceNet_862\n",
            "DogFaceNet_88\n",
            "DogFaceNet_885\n",
            "DogFaceNet_899\n",
            "DogFaceNet_901\n",
            "DogFaceNet_904\n",
            "DogFaceNet_905\n",
            "DogFaceNet_909\n",
            "DogFaceNet_954\n",
            "DogFaceNet_976\n",
            "DogFaceNet_986\n",
            "FriesianCattle2015_18\n",
            "FriesianCattle2015_38\n",
            "FriesianCattle2017_46\n",
            "FriesianCattle2017_50\n",
            "FriesianCattle2017_53\n",
            "FriesianCattle2017_8\n",
            "GiraffeZebraID_IBEIS_PZ_0010\n",
            "GiraffeZebraID_IBEIS_PZ_0034\n",
            "GiraffeZebraID_IBEIS_PZ_0044\n",
            "GiraffeZebraID_IBEIS_PZ_0064\n",
            "GiraffeZebraID_IBEIS_PZ_0084\n",
            "GiraffeZebraID_IBEIS_PZ_0100\n",
            "GiraffeZebraID_IBEIS_PZ_0112\n",
            "GiraffeZebraID_IBEIS_PZ_0135\n",
            "GiraffeZebraID_IBEIS_PZ_0137\n",
            "GiraffeZebraID_IBEIS_PZ_0141\n",
            "GiraffeZebraID_IBEIS_PZ_0156\n",
            "GiraffeZebraID_IBEIS_PZ_0188\n",
            "GiraffeZebraID_IBEIS_PZ_0222\n",
            "GiraffeZebraID_IBEIS_PZ_0223\n",
            "GiraffeZebraID_IBEIS_PZ_0232\n",
            "GiraffeZebraID_IBEIS_PZ_0262\n",
            "GiraffeZebraID_IBEIS_PZ_0304\n",
            "GiraffeZebraID_IBEIS_PZ_0360\n",
            "GiraffeZebraID_IBEIS_PZ_0363\n",
            "GiraffeZebraID_IBEIS_PZ_0373\n",
            "GiraffeZebraID_IBEIS_PZ_0396\n",
            "GiraffeZebraID_IBEIS_PZ_0400\n",
            "GiraffeZebraID_IBEIS_PZ_0431\n",
            "GiraffeZebraID_IBEIS_PZ_0455\n",
            "GiraffeZebraID_IBEIS_PZ_0464\n",
            "GiraffeZebraID_IBEIS_PZ_0502\n",
            "GiraffeZebraID_IBEIS_PZ_0528\n",
            "GiraffeZebraID_IBEIS_PZ_0536\n",
            "GiraffeZebraID_IBEIS_PZ_0537\n",
            "GiraffeZebraID_IBEIS_PZ_0555\n",
            "GiraffeZebraID_IBEIS_PZ_0556\n",
            "GiraffeZebraID_IBEIS_PZ_0588\n",
            "GiraffeZebraID_IBEIS_PZ_0599\n",
            "GiraffeZebraID_IBEIS_PZ_0721\n",
            "GiraffeZebraID_IBEIS_PZ_0750\n",
            "GiraffeZebraID_IBEIS_PZ_0758\n",
            "GiraffeZebraID_IBEIS_PZ_0809\n",
            "GiraffeZebraID_IBEIS_PZ_0831\n",
            "GiraffeZebraID_IBEIS_PZ_0883\n",
            "GiraffeZebraID_IBEIS_PZ_0892\n",
            "GiraffeZebraID_IBEIS_PZ_0899\n",
            "GiraffeZebraID_IBEIS_PZ_0941\n",
            "GiraffeZebraID_IBEIS_PZ_0952\n",
            "GiraffeZebraID_IBEIS_PZ_0954\n",
            "GiraffeZebraID_IBEIS_PZ_0971\n",
            "GiraffeZebraID_IBEIS_PZ_0990\n",
            "GiraffeZebraID_IBEIS_PZ_1028\n",
            "GiraffeZebraID_IBEIS_PZ_1030\n",
            "GiraffeZebraID_IBEIS_PZ_1054\n",
            "GiraffeZebraID_IBEIS_PZ_1136\n",
            "GiraffeZebraID_IBEIS_PZ_1142\n",
            "GiraffeZebraID_IBEIS_PZ_1144\n",
            "GiraffeZebraID_IBEIS_PZ_1145\n",
            "GiraffeZebraID_IBEIS_PZ_1161\n",
            "GiraffeZebraID_IBEIS_PZ_1170\n",
            "GiraffeZebraID_IBEIS_PZ_1173\n",
            "GiraffeZebraID_IBEIS_PZ_1185\n",
            "GiraffeZebraID_IBEIS_PZ_1192\n",
            "GiraffeZebraID_IBEIS_PZ_1229\n",
            "GiraffeZebraID_IBEIS_PZ_1230\n",
            "GiraffeZebraID_IBEIS_PZ_1240\n",
            "GiraffeZebraID_IBEIS_PZ_1247\n",
            "GiraffeZebraID_IBEIS_PZ_1263\n",
            "GiraffeZebraID_IBEIS_PZ_1287\n",
            "GiraffeZebraID_IBEIS_PZ_1322\n",
            "GiraffeZebraID_IBEIS_PZ_1326\n",
            "GiraffeZebraID_IBEIS_PZ_1330\n",
            "GiraffeZebraID_IBEIS_PZ_1348\n",
            "GiraffeZebraID_IBEIS_PZ_1396\n",
            "GiraffeZebraID_IBEIS_PZ_1402\n",
            "GiraffeZebraID_IBEIS_PZ_1403\n",
            "GiraffeZebraID_IBEIS_PZ_1413\n",
            "GiraffeZebraID_IBEIS_PZ_1460\n",
            "GiraffeZebraID_IBEIS_PZ_1462\n",
            "GiraffeZebraID_IBEIS_PZ_1485\n",
            "GiraffeZebraID_IBEIS_PZ_1512\n",
            "GiraffeZebraID_IBEIS_PZ_1549\n",
            "GiraffeZebraID_IBEIS_PZ_1557\n",
            "GiraffeZebraID_IBEIS_PZ_1599\n",
            "GiraffeZebraID_IBEIS_PZ_1647\n",
            "GiraffeZebraID_IBEIS_PZ_1677\n",
            "GiraffeZebraID_IBEIS_PZ_1697\n",
            "GiraffeZebraID_IBEIS_PZ_1737\n",
            "GiraffeZebraID_IBEIS_PZ_1749\n",
            "GiraffeZebraID_IBEIS_PZ_1763\n",
            "GiraffeZebraID_IBEIS_PZ_1789\n",
            "GiraffeZebraID_IBEIS_PZ_1793\n",
            "GiraffeZebraID_IBEIS_PZ_1826\n",
            "GiraffeZebraID_IBEIS_PZ_1850\n",
            "GiraffeZebraID_IBEIS_PZ_1855\n",
            "GiraffeZebraID_IBEIS_PZ_1869\n",
            "GiraffeZebraID_IBEIS_PZ_1902\n",
            "GiraffeZebraID_NNP_GIRM_0039\n",
            "GiraffeZebraID_NNP_GIRM_0041\n",
            "GiraffeZebraID_NNP_GIRM_0059\n",
            "GiraffeZebraID_NNP_GIRM_0079\n",
            "GiraffeZebraID_NNP_GIRM_0080\n",
            "GiraffeZebraID_NNP_GIRM_0081\n",
            "GiraffeZebraID_NNP_GIRM_0115\n",
            "GiraffeZebraID_NNP_GIRM_0121\n",
            "GiraffeZebraID_NNP_GIRM_0125\n",
            "GiraffeZebraID_NNP_GIRM_0136\n",
            "GiraffeZebraID_NNP_GIRM_0143\n",
            "Giraffes_cluster1135\n",
            "Giraffes_cluster1242\n",
            "Giraffes_cluster164.01\n",
            "Giraffes_cluster234\n",
            "Giraffes_cluster423\n",
            "Giraffes_cluster566\n",
            "Giraffes_cluster872\n",
            "Giraffes_cluster9.01\n",
            "HyenaID2022_1848c437-2486-4485-a725-f1db51065599\n",
            "HyenaID2022_310eb8ee-7097-4ea5-b91e-31ce40b9685c\n",
            "HyenaID2022_3be4ffda-e5d4-4ff8-bed2-da0417e3636f\n",
            "HyenaID2022_61f3efff-c86f-48ed-8b36-0dec4078b231\n",
            "HyenaID2022_7b38f393-27e8-45e1-9769-fa9af9f591f9\n",
            "HyenaID2022_89390731-0ff1-454f-a2ac-8017c49d8da6\n",
            "HyenaID2022_8b046377-529a-460d-8114-887322c52cde\n",
            "HyenaID2022_bc95fccf-3431-4dee-b23f-c0a10f19b05c\n",
            "HyenaID2022_c6241bf0-3991-48e7-862b-052e4c1eef69\n",
            "IPanda50_02_baolan\n",
            "IPanda50_22_nini\n",
            "LeopardID2022_14a1f572-2a8a-4f98-b740-0d6fb9b458e6\n",
            "LeopardID2022_1e4941f3-fd74-4119-8dc7-120249cb9e76\n",
            "LeopardID2022_1f1803f4-f92d-459c-9662-c3e2c558be0b\n",
            "LeopardID2022_1f68f464-2c83-4a4b-9449-172d7b700d02\n",
            "LeopardID2022_2a171d14-e10a-44d2-97cc-14865f315bb4\n",
            "LeopardID2022_5d4eb46f-f5f8-418f-bbf9-00007edae5c8\n",
            "LeopardID2022_6ef1e021-1210-4ddd-a9ce-a64d8a8f50fe\n",
            "LeopardID2022_73fba6b6-232c-4160-a46c-5f85a894a9db\n",
            "LeopardID2022_8ec2a1cc-fac7-4b7e-bcb0-4ce4d03be451\n",
            "LeopardID2022_8f76acd2-f3ba-4c7a-a00f-bb71343d2356\n",
            "LeopardID2022_92979236-b885-4aa2-8f22-da447159f85a\n",
            "LeopardID2022_951f3bbf-f9f1-4ace-ab20-ef153169384f\n",
            "LeopardID2022_9529feef-a4af-4783-bc46-3b8c317a3da6\n",
            "LeopardID2022_9a3319f8-f480-4cf0-b917-32bb8998f81b\n",
            "LeopardID2022_a8a4188d-54ed-416b-820b-20086e946524\n",
            "LeopardID2022_e3c1078a-aaed-42f2-85ae-e1d3b618cdee\n",
            "LeopardID2022_fe0d7380-1d87-48c5-851a-24e6484f0094\n",
            "MPDD_11\n",
            "MPDD_118\n",
            "MPDD_13\n",
            "MPDD_14\n",
            "MPDD_142\n",
            "MPDD_181\n",
            "MPDD_185\n",
            "MPDD_5\n",
            "MPDD_72\n",
            "MultiCamCows2024_26\n",
            "MultiCamCows2024_29\n",
            "MultiCamCows2024_61\n",
            "MultiCamCows2024_86\n",
            "NDD20_23\n",
            "NDD20_41\n",
            "NDD20_53\n",
            "NDD20_59\n",
            "NyalaData_131\n",
            "NyalaData_144\n",
            "NyalaData_146\n",
            "NyalaData_179\n",
            "NyalaData_19\n",
            "NyalaData_197\n",
            "NyalaData_207\n",
            "NyalaData_213\n",
            "NyalaData_5\n",
            "NyalaData_81\n",
            "OpenCows2020_26\n",
            "OpenCows2020_6\n",
            "PolarBearVidID_Vitus\n",
            "PrimFace_j02\n",
            "PrimFace_r02\n",
            "PrimFace_r32\n",
            "ReunionTurtles_Calvin\n",
            "ReunionTurtles_Ipanema\n",
            "ReunionTurtles_May-Lee\n",
            "ReunionTurtles_Sablier\n",
            "ReunionTurtles_Yanis\n",
            "SMALST_I\n",
            "SeaStarReID2023_Anau38\n",
            "SeaStarReID2023_Asru50\n",
            "SeaStarReID2023_Asru52\n",
            "SeaStarReID2023_Asru60\n",
            "SeaTurtleID2022_t028\n",
            "SeaTurtleID2022_t029\n",
            "SeaTurtleID2022_t061\n",
            "SeaTurtleID2022_t065\n",
            "SeaTurtleID2022_t077\n",
            "SeaTurtleID2022_t083\n",
            "SeaTurtleID2022_t090\n",
            "SeaTurtleID2022_t092\n",
            "SeaTurtleID2022_t101\n",
            "SeaTurtleID2022_t137\n",
            "SeaTurtleID2022_t155\n",
            "SeaTurtleID2022_t181\n",
            "SeaTurtleID2022_t221\n",
            "SeaTurtleID2022_t371\n",
            "SeaTurtleID2022_t375\n",
            "SeaTurtleID2022_t381\n",
            "SeaTurtleID2022_t384\n",
            "SeaTurtleID2022_t387\n",
            "SeaTurtleID2022_t392\n",
            "SeaTurtleID2022_t406\n",
            "SeaTurtleID2022_t435\n",
            "SeaTurtleID2022_t468\n",
            "SeaTurtleID2022_t470\n",
            "SealID_103\n",
            "SealID_167\n",
            "SouthernProvinceTurtles_12-2-6\n",
            "SouthernProvinceTurtles_14-2-1\n",
            "SouthernProvinceTurtles_15-2-15\n",
            "StripeSpotter_S09_111\n",
            "StripeSpotter_S09_161\n",
            "WhaleSharkID_096eb7d9-82e3-d5a5-1116-42ddd304701e\n",
            "WhaleSharkID_184e5bde-0e49-4b81-bd5b-a85e276a593e\n",
            "WhaleSharkID_24dd4346-fae4-97e7-61e1-acd06f4fedfc\n",
            "WhaleSharkID_28f11ea8-d7e7-2429-e62f-ed5387357fc7\n",
            "WhaleSharkID_30f4b208-45db-55ba-da2f-6ed0699d59e0\n",
            "WhaleSharkID_39c01a38-f68a-743b-937d-6f5d1a5fd315\n",
            "WhaleSharkID_5567fe79-b98d-e325-0990-02c0cd54c38e\n",
            "WhaleSharkID_6e7973e3-ac8d-f2ff-8c47-b02de172a80b\n",
            "WhaleSharkID_71039af3-3766-b597-d579-ae26cf995a9a\n",
            "WhaleSharkID_781ee8dd-8d83-1974-65d0-3f86de71801a\n",
            "WhaleSharkID_7c8a623d-c9cf-0844-f844-b2ed52063b87\n",
            "WhaleSharkID_9124cef3-4214-9f9e-12ed-f07bd9deaa3e\n",
            "WhaleSharkID_93b2cbb3-bfc6-e7c3-ce4b-4118a0ceff8f\n",
            "WhaleSharkID_9af0c423-b335-e1a4-cbe5-498b97678f09\n",
            "WhaleSharkID_a4a0c373-bb70-5970-be1b-64429fae94b6\n",
            "WhaleSharkID_b11b8c73-78e4-1cb6-6260-c68e4264f9c4\n",
            "WhaleSharkID_b2ec24c6-f18a-967f-ada6-4248b774efa0\n",
            "WhaleSharkID_b7707e88-c560-eb8c-196d-c6d29078316e\n",
            "WhaleSharkID_d5715a5e-79ac-6d84-b946-39bcb8085a6f\n",
            "WhaleSharkID_d6f53183-2f77-7326-07d5-062b31d1fb7b\n",
            "WhaleSharkID_e125d3f5-fd3f-af8a-42f7-5cf01ae299a7\n",
            "WhaleSharkID_e51ca99b-7f28-af3c-2fde-51e70deb5ab7\n",
            "WhaleSharkID_f63de316-93c6-ca4b-6d79-3955e6812aac\n",
            "WhaleSharkID_faa5d22c-f587-6772-986d-d2ef583d9ec2\n",
            "ZakynthosTurtles_G20-05\n",
            "ZakynthosTurtles_t573\n",
            "ZindiTurtleRecall_t_id_0FcGb4xg\n",
            "ZindiTurtleRecall_t_id_0MlFQ8pw\n",
            "ZindiTurtleRecall_t_id_0xZ0aGD9\n",
            "ZindiTurtleRecall_t_id_12hGhewc\n",
            "ZindiTurtleRecall_t_id_15bo4NKD\n",
            "ZindiTurtleRecall_t_id_1voKhzZi\n",
            "ZindiTurtleRecall_t_id_2jJNiPzx\n",
            "ZindiTurtleRecall_t_id_31E0Dcfq\n",
            "ZindiTurtleRecall_t_id_3fKmnkBS\n",
            "ZindiTurtleRecall_t_id_3zImrqlW\n",
            "ZindiTurtleRecall_t_id_4CwFTbcy\n",
            "ZindiTurtleRecall_t_id_5XNXqlqy\n",
            "ZindiTurtleRecall_t_id_6HjuiTBl\n",
            "ZindiTurtleRecall_t_id_6PFhdt0v\n",
            "ZindiTurtleRecall_t_id_6gm9eboc\n",
            "ZindiTurtleRecall_t_id_75sE0fQK\n",
            "ZindiTurtleRecall_t_id_7Qo68L4D\n",
            "ZindiTurtleRecall_t_id_80O3cONJ\n",
            "ZindiTurtleRecall_t_id_88boh82G\n",
            "ZindiTurtleRecall_t_id_9M3d0yBk\n",
            "ZindiTurtleRecall_t_id_9QGx0oP6\n",
            "ZindiTurtleRecall_t_id_9cyNVfJk\n",
            "ZindiTurtleRecall_t_id_9fzItukM\n",
            "ZindiTurtleRecall_t_id_Af7FcNbg\n",
            "ZindiTurtleRecall_t_id_BEQAtISc\n",
            "ZindiTurtleRecall_t_id_BaGSofg0\n",
            "ZindiTurtleRecall_t_id_BuO0n1wz\n",
            "ZindiTurtleRecall_t_id_BzHOyN1w\n",
            "ZindiTurtleRecall_t_id_DKgLDUiB\n",
            "ZindiTurtleRecall_t_id_DPRWRGnW\n",
            "ZindiTurtleRecall_t_id_ES1owbvW\n",
            "ZindiTurtleRecall_t_id_EWy4fMfl\n",
            "ZindiTurtleRecall_t_id_ElPRlKY1\n",
            "ZindiTurtleRecall_t_id_ElTtNp9g\n",
            "ZindiTurtleRecall_t_id_FClu4yCG\n",
            "ZindiTurtleRecall_t_id_FYlrImRy\n",
            "ZindiTurtleRecall_t_id_FzFIwe6I\n",
            "ZindiTurtleRecall_t_id_HTRJc5Mm\n",
            "ZindiTurtleRecall_t_id_JI3lt9E9\n",
            "ZindiTurtleRecall_t_id_JJ7MeHZj\n",
            "ZindiTurtleRecall_t_id_Ju3Z38U6\n",
            "ZindiTurtleRecall_t_id_K8mTTV7U\n",
            "ZindiTurtleRecall_t_id_KBvmODkY\n",
            "ZindiTurtleRecall_t_id_KUdlV6Iv\n",
            "ZindiTurtleRecall_t_id_LROcN6zI\n",
            "ZindiTurtleRecall_t_id_LwTnpSIH\n",
            "ZindiTurtleRecall_t_id_O2pRMC2d\n",
            "ZindiTurtleRecall_t_id_O8NKiuJ8\n",
            "ZindiTurtleRecall_t_id_OH5LOWdR\n",
            "ZindiTurtleRecall_t_id_OH7Q4Vo1\n",
            "ZindiTurtleRecall_t_id_OpqNDYHN\n",
            "ZindiTurtleRecall_t_id_P7iHYwoE\n",
            "ZindiTurtleRecall_t_id_PAnpkKwb\n",
            "ZindiTurtleRecall_t_id_PMbxeUCr\n",
            "ZindiTurtleRecall_t_id_QJxK2Cu5\n",
            "ZindiTurtleRecall_t_id_QrKns97J\n",
            "ZindiTurtleRecall_t_id_QuB6hc4M\n",
            "ZindiTurtleRecall_t_id_RDv3jcgj\n",
            "ZindiTurtleRecall_t_id_RQ6v2yAg\n",
            "ZindiTurtleRecall_t_id_SAZJLKUz\n",
            "ZindiTurtleRecall_t_id_SQ7isMue\n",
            "ZindiTurtleRecall_t_id_Sds3jHDU\n",
            "ZindiTurtleRecall_t_id_Se3dSgIY\n",
            "ZindiTurtleRecall_t_id_SrdYWsBp\n",
            "ZindiTurtleRecall_t_id_TDt3UTec\n",
            "ZindiTurtleRecall_t_id_U22A48Kn\n",
            "ZindiTurtleRecall_t_id_UCoQ8zp2\n",
            "ZindiTurtleRecall_t_id_UjNkSw6U\n",
            "ZindiTurtleRecall_t_id_V9W5yykB\n",
            "ZindiTurtleRecall_t_id_VFb44eFm\n",
            "ZindiTurtleRecall_t_id_W4PcCSsv\n",
            "ZindiTurtleRecall_t_id_WCLFdbn1\n",
            "ZindiTurtleRecall_t_id_WdKNC0I6\n",
            "ZindiTurtleRecall_t_id_Wy3OlPbq\n",
            "ZindiTurtleRecall_t_id_X4MMVz4b\n",
            "ZindiTurtleRecall_t_id_Xs8emhVq\n",
            "ZindiTurtleRecall_t_id_ZDS7oEep\n",
            "ZindiTurtleRecall_t_id_ZLqPpaZD\n",
            "ZindiTurtleRecall_t_id_ZMLjjpoI\n",
            "ZindiTurtleRecall_t_id_ZXKaunaj\n",
            "ZindiTurtleRecall_t_id_ZbB9UpUH\n",
            "ZindiTurtleRecall_t_id_ZdqTzVru\n",
            "ZindiTurtleRecall_t_id_a4VYrmyA\n",
            "ZindiTurtleRecall_t_id_at1UMrxB\n",
            "ZindiTurtleRecall_t_id_cSbCd2KK\n",
            "ZindiTurtleRecall_t_id_dYntefAu\n",
            "ZindiTurtleRecall_t_id_euE83bjc\n",
            "ZindiTurtleRecall_t_id_hRqnvc5Q\n",
            "ZindiTurtleRecall_t_id_hiKbfs3z\n",
            "ZindiTurtleRecall_t_id_hvSrfs4e\n",
            "ZindiTurtleRecall_t_id_j50ldCyT\n",
            "ZindiTurtleRecall_t_id_jE2gZThG\n",
            "ZindiTurtleRecall_t_id_jGKHCNjF\n",
            "ZindiTurtleRecall_t_id_kCmtbVnc\n",
            "ZindiTurtleRecall_t_id_kN0ca68K\n",
            "ZindiTurtleRecall_t_id_ksTLswDT\n",
            "ZindiTurtleRecall_t_id_kzNpjPcj\n",
            "ZindiTurtleRecall_t_id_l8peB2JQ\n",
            "ZindiTurtleRecall_t_id_lCCo4p0b\n",
            "ZindiTurtleRecall_t_id_lDzaPi56\n",
            "ZindiTurtleRecall_t_id_nZgUJ2oa\n",
            "ZindiTurtleRecall_t_id_o8HFaaCp\n",
            "ZindiTurtleRecall_t_id_pRIKaLXT\n",
            "ZindiTurtleRecall_t_id_pf4YVjs0\n",
            "ZindiTurtleRecall_t_id_qg4Do3Ze\n",
            "ZindiTurtleRecall_t_id_tBxEKReV\n",
            "ZindiTurtleRecall_t_id_tZYtoMIR\n",
            "ZindiTurtleRecall_t_id_vcjEyexj\n",
            "ZindiTurtleRecall_t_id_wMZesE3X\n",
            "ZindiTurtleRecall_t_id_x0N1mhsU\n",
            "ZindiTurtleRecall_t_id_x7jegLhi\n",
            "ZindiTurtleRecall_t_id_yMPeFkMB\n",
            "ZindiTurtleRecall_t_id_ydgdWdcM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load the database_metadata.csv\n",
        "database_df = pd.read_csv('metadata.csv')\n",
        "\n",
        "# 2. Find identities that appear only once\n",
        "identity_counts = database_df['identity'].value_counts()\n",
        "single_occurrence_identities = identity_counts[identity_counts == 1].index\n",
        "\n",
        "# 3. Split the data:\n",
        "# - Images with identities appearing only once\n",
        "single_occurrence_df = database_df[database_df['identity'].isin(single_occurrence_identities)]\n",
        "print(\"single_occurrence_df:\")\n",
        "single_occurrence_df.info()\n",
        "print()\n",
        "\n",
        "# - Images with identities appearing at least twice\n",
        "normal_df = database_df[~database_df['identity'].isin(single_occurrence_identities)]\n",
        "print(\"normal_df:\")\n",
        "normal_df.info()\n",
        "print()\n",
        "\n",
        "# 4. Perform stratified train/validation split on normal identities\n",
        "train_normal_df, test_normal_df = train_test_split(\n",
        "    normal_df,\n",
        "    test_size=0.2,\n",
        "    stratify=normal_df['identity'],\n",
        "    random_state=42\n",
        ")\n",
        "print(\"train_normal.df:\")\n",
        "train_normal_df.info()\n",
        "print()\n",
        "print(\"test_normal.df:\")\n",
        "test_normal_df.info()\n",
        "print()\n",
        "\n",
        "# 5. Combine all single-occurrence identities + 80% of normal identities into the training set\n",
        "train_df = pd.concat([train_normal_df, single_occurrence_df]).reset_index(drop=True)\n",
        "\n",
        "# 6. Save the CSV files\n",
        "train_df.to_csv('train_metadata.csv', index=False)\n",
        "test_normal_df.to_csv('test_metadata.csv', index=False)\n",
        "\n",
        "print(f\"Train set: {len(train_df)} images\")\n",
        "print(f\"Test set: {len(test_normal_df)} images\")\n",
        "split = len(test_normal_df)/len(train_df)\n",
        "print(f\"Split: {split}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8OeZ-fhCbSW",
        "outputId": "4bab7f65-482f-48d8-8639-5857bd5e5f27",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6a2a8bb70a60>:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  database_df = pd.read_csv('metadata.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "single_occurrence_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1019 entries, 40128 to 138026\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   identity     1019 non-null   object\n",
            " 1   path         1019 non-null   object\n",
            " 2   date         912 non-null    object\n",
            " 3   orientation  993 non-null    object\n",
            " 4   species      1019 non-null   object\n",
            " 5   split        1019 non-null   object\n",
            " 6   dataset      1019 non-null   object\n",
            " 7   cluster_id   911 non-null    object\n",
            "dtypes: object(8)\n",
            "memory usage: 71.6+ KB\n",
            "\n",
            "normal_df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 139469 entries, 0 to 140487\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   identity     139469 non-null  object\n",
            " 1   path         139469 non-null  object\n",
            " 2   date         46940 non-null   object\n",
            " 3   orientation  49524 non-null   object\n",
            " 4   species      139469 non-null  object\n",
            " 5   split        139469 non-null  object\n",
            " 6   dataset      139469 non-null  object\n",
            " 7   cluster_id   71698 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 9.6+ MB\n",
            "\n",
            "train_normal.df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 111575 entries, 98906 to 77638\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count   Dtype \n",
            "---  ------       --------------   ----- \n",
            " 0   identity     111575 non-null  object\n",
            " 1   path         111575 non-null  object\n",
            " 2   date         37680 non-null   object\n",
            " 3   orientation  39768 non-null   object\n",
            " 4   species      111575 non-null  object\n",
            " 5   split        111575 non-null  object\n",
            " 6   dataset      111575 non-null  object\n",
            " 7   cluster_id   57593 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 7.7+ MB\n",
            "\n",
            "test_normal.df:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 27894 entries, 129923 to 42337\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   identity     27894 non-null  object\n",
            " 1   path         27894 non-null  object\n",
            " 2   date         9260 non-null   object\n",
            " 3   orientation  9756 non-null   object\n",
            " 4   species      27894 non-null  object\n",
            " 5   split        27894 non-null  object\n",
            " 6   dataset      27894 non-null  object\n",
            " 7   cluster_id   14105 non-null  object\n",
            "dtypes: object(8)\n",
            "memory usage: 1.9+ MB\n",
            "\n",
            "Train set: 112594 images\n",
            "Test set: 27894 images\n",
            "Split: 0.24773966641206457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Dataloaders**"
      ],
      "metadata": {
        "id": "qdfcgZ_zMFfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile create_dataloaders.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "        self.label_decoder = {v: k for k, v in label_encoder.items()}  # reverse mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        identity = self.data.iloc[idx]['identity']\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.label_encoder[identity]\n",
        "\n",
        "        return image, label, img_path, identity  # return more info\n",
        "\n",
        "# Root directory where images are located\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Simple resize for displaying images\n",
        "transform_display = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "])\n",
        "\n",
        "transform = T.Compose([\n",
        "    *transform_display.transforms,\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Datasets\n",
        "train_dataset = AnimalDataset(\n",
        "    csv_file=train_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = AnimalDataset(\n",
        "    csv_file=test_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, labels, paths, identities = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    labels = torch.tensor(labels)\n",
        "    return images, labels, paths, identities\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "images, labels, paths, identities = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nTrain batch - images shape: {images.shape}\")\n",
        "print(f\"Train batch - labels shape: {labels.shape}\")\n",
        "print(f\"Train batch - labels example: {labels[:5]}\")\n",
        "\n",
        "for i in range(len(paths)):\n",
        "    print(f\"[Train] Image Path: {paths[i]} | Label ID: {labels[i].item()} | Identity: {identities[i]}\")\n",
        "\n",
        "images_test, labels_test, paths_test, identities_test = next(iter(test_loader))\n",
        "\n",
        "print(f\"\\nTest batch - images shape: {images_test.shape}\")\n",
        "print(f\"Test batch - labels shape: {labels_test.shape}\")\n",
        "print(f\"Test batch - labels example: {labels_test[:5]}\")\n",
        "\n",
        "for i in range(len(paths_test)):\n",
        "    print(f\"[Test] Image Path: {paths_test[i]} | Label ID: {labels_test[i].item()} | Identity: {identities_test[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0ZPst1vQ-3Q",
        "outputId": "cf19ff92-c1e1-4146-96e8-f1949971c750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting create_dataloaders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile create_dataloaders.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Build the full path to the image\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Label is the value from \"identity\"\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Root directory where images are located\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Simple resize for displaying images\n",
        "transform_display = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "])\n",
        "\n",
        "# Transform for training / evaluation (resize + normalization)\n",
        "transform = T.Compose([\n",
        "    *transform_display.transforms,   # reuse resize step\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Datasets\n",
        "train_dataset = AnimalDataset(\n",
        "    csv_file=train_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = AnimalDataset(\n",
        "    csv_file=test_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "print(f\"\\nTrain batch - images shape: {images.shape}\")\n",
        "print(f\"Train batch - labels shape: {labels.shape}\")\n",
        "print(f\"Train batch - labels example: {labels[:5]}\")\n",
        "\n",
        "images_test, labels_test = next(iter(test_loader))\n",
        "\n",
        "print(f\"\\nTest batch - images shape: {images_test.shape}\")\n",
        "print(f\"Test batch - labels shape: {labels_test.shape}\")\n",
        "print(f\"Test batch - labels example: {labels_test[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iasypuG8_iTF",
        "outputId": "a728985b-0cb6-467d-973e-1a2861189acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing create_dataloaders.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tune**"
      ],
      "metadata": {
        "id": "XdPo24hwTS_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fine_tune_animal_clef.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "import timm\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Build the full path to the image\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Label is the value from \"identity\"\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0005):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Training (Epoch {epoch+1})\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Validating (Epoch {epoch+1})\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n",
        "\n",
        "# Root directory where images are located\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Transforms for train and validation\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=10),\n",
        "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.33))\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Datasets\n",
        "train_dataset = AnimalDataset(\n",
        "    csv_file=train_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = AnimalDataset(\n",
        "    csv_file=test_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "model = timm.create_model(\n",
        "    #'vit_large_patch16_384',\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "    #'swin_large_patch4_window12_384',\n",
        "    pretrained=True,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 30\n",
        "lr = 8e-5\n",
        "weight_decay = 0.01\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "metrics_csv = \"epoch_metrics.csv\"\n",
        "\n",
        "if not os.path.exists(metrics_csv):\n",
        "    with open(metrics_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\", \"learning_rate\"])\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "early_stopping = EarlyStopping(patience=8, min_delta=0.0005)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch)\n",
        "    val_loss, val_acc = validate(model, test_loader, criterion, device, epoch)\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f} | \"\n",
        "          f\"LR: {current_lr:.8f}\")\n",
        "\n",
        "    with open(metrics_csv, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch+1, train_loss, val_loss, val_acc, current_lr])\n",
        "\n",
        "    # Save best model if val_loss decreased and val_acc increased\n",
        "    if val_loss < best_val_loss and val_acc > best_val_acc:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'label_encoder': label_encoder,\n",
        "        }, best_model_path)\n",
        "        print(f\"✅ Best model saved at epoch {epoch+1} with Val Loss {val_loss:.4f} and Val Acc {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.should_stop(val_loss):\n",
        "        print(\"🛑 Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Plot Train and Validation Loss\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.savefig(\"loss_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Learning Rate over Epochs\n",
        "plt.plot(learning_rates, label=\"Learning Rate\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Rate Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.grid()\n",
        "plt.savefig(\"learning_rate_plot.png\")\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnPz_AaCadF8",
        "outputId": "2ef11861-2288-4455-dac1-860fe0c47790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fine_tune_animal_clef.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fine_tune_animal_clef.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "import timm\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Build the full path to the image\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Label is the value from \"identity\"\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0005):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Training (Epoch {epoch+1})\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Validating (Epoch {epoch+1})\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n",
        "\n",
        "class GLUBlock(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(dim_in, dim_out * 2)\n",
        "        self.norm = nn.LayerNorm(dim_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_proj = self.linear(x)\n",
        "        out, gate = x_proj.chunk(2, dim=-1)\n",
        "        return self.norm(out * torch.sigmoid(gate))\n",
        "\n",
        "class MultiBackboneClassifier(nn.Module):\n",
        "    def __init__(self, model_names, embedding_dims, num_classes):\n",
        "        super().__init__()\n",
        "        assert len(model_names) == len(embedding_dims)\n",
        "\n",
        "        self.backbones = nn.ModuleList([\n",
        "            timm.create_model(name, pretrained=True, num_classes=0)\n",
        "            for name in model_names\n",
        "        ])\n",
        "\n",
        "        total_dim = sum(embedding_dims)\n",
        "\n",
        "        total_dim = sum(embedding_dims)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(total_dim),\n",
        "            GLUBlock(total_dim, 2048, dropout=0.4),\n",
        "            GLUBlock(2048, 1024, dropout=0.3),\n",
        "            GLUBlock(1024, 512, dropout=0.2),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = [F.normalize(backbone(x), p=2, dim=1) for backbone in self.backbones]\n",
        "        fused = torch.cat(feats, dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        log_probs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -log_probs.mean(dim=-1)\n",
        "        return ((1.0 - self.smoothing) * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "\n",
        "def freeze_backbones(model):\n",
        "    for backbone in model.backbones:\n",
        "        for param in backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "def unfreeze_backbones(model):\n",
        "    for backbone in model.backbones:\n",
        "        for param in backbone.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "# Root directory where images are located\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Transforms for train and validation\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=10),\n",
        "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.33))\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Datasets\n",
        "train_dataset = AnimalDataset(\n",
        "    csv_file=train_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = AnimalDataset(\n",
        "    csv_file=test_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "model_names = [\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',   # 2048\n",
        "    'swin_large_patch4_window12_384',         # 1536\n",
        "    'maxvit_xlarge_tf_384.in21k_ft_in1k'      # 2048\n",
        "]\n",
        "\n",
        "embedding_dims = [2048, 1536, 2048]\n",
        "\n",
        "model = MultiBackboneClassifier(\n",
        "    model_names=model_names,\n",
        "    embedding_dims=embedding_dims,\n",
        "    num_classes=len(label_encoder)\n",
        ").to(device)\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=8e-5, weight_decay=0.01)\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "metrics_csv = \"epoch_metrics.csv\"\n",
        "\n",
        "if not os.path.exists(metrics_csv):\n",
        "    with open(metrics_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\", \"learning_rate\"])\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "early_stopping = EarlyStopping(patience=8, min_delta=0.0005)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch)\n",
        "    val_loss, val_acc = validate(model, test_loader, criterion, device, epoch)\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f} | \"\n",
        "          f\"LR: {current_lr:.8f}\")\n",
        "\n",
        "    with open(metrics_csv, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch+1, train_loss, val_loss, val_acc, current_lr])\n",
        "\n",
        "    # Save best model if val_loss decreased and val_acc increased\n",
        "    if val_loss < best_val_loss and val_acc > best_val_acc:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'label_encoder': label_encoder,\n",
        "        }, best_model_path)\n",
        "        print(f\"✅ Best model saved at epoch {epoch+1} with Val Loss {val_loss:.4f} and Val Acc {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.should_stop(val_loss):\n",
        "        print(\"🛑 Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Plot Train and Validation Loss\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.savefig(\"loss_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Learning Rate over Epochs\n",
        "plt.plot(learning_rates, label=\"Learning Rate\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Rate Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.grid()\n",
        "plt.savefig(\"learning_rate_plot.png\")\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO-dq5bT2ZO8",
        "outputId": "7abfa012-fe59-449c-b71a-cacb994ae4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fine_tune_animal_clef.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fine_tune_animal_clef.py\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "import timm\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Build the full path to the image\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations (if any)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Label is the value from \"identity\"\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0005):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "class EnsembleNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone1 = timm.create_model(\n",
        "            'swin_large_patch4_window12_384',\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "        self.backbone2 = timm.create_model(\n",
        "            'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "        self.backbone3 = timm.create_model(\n",
        "            'beit_large_patch16_384.in22k_ft_in22k_in1k',\n",
        "            pretrained=True,\n",
        "            num_classes=0\n",
        "        )\n",
        "\n",
        "        self.feat_dim1 = self.backbone1.num_features\n",
        "        self.feat_dim2 = self.backbone2.num_features\n",
        "        self.feat_dim3 = self.backbone3.num_features\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.feat_dim1 + self.feat_dim2 + self.feat_dim3),\n",
        "            nn.Linear(self.feat_dim1 + self.feat_dim2 + self.feat_dim3, 4096),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(2048, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f1 = self.backbone1(x)\n",
        "        f2 = self.backbone2(x)\n",
        "        f3 = self.backbone3(x)\n",
        "        fused = torch.cat([f1, f2, f3], dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Training (Epoch {epoch+1})\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Validating (Epoch {epoch+1})\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n",
        "\n",
        "# Root directory where images are located\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Transforms for train and validation\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=10),\n",
        "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    T.RandomErasing(p=0.25, scale=(0.02, 0.33))\n",
        "])\n",
        "\n",
        "test_transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Datasets\n",
        "train_dataset = AnimalDataset(\n",
        "    csv_file=train_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "test_dataset = AnimalDataset(\n",
        "    csv_file=test_csv,\n",
        "    root_dir=root,\n",
        "    label_encoder=label_encoder,\n",
        "    transform=test_transform\n",
        ")\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "model = EnsembleNet(num_classes=len(label_encoder)).to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 30\n",
        "lr = 2e-5\n",
        "weight_decay = 0.01\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "metrics_csv = \"epoch_metrics.csv\"\n",
        "\n",
        "if not os.path.exists(metrics_csv):\n",
        "    with open(metrics_csv, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"val_acc\", \"learning_rate\"])\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "early_stopping = EarlyStopping(patience=8, min_delta=0.0005)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device, epoch)\n",
        "    val_loss, val_acc = validate(model, test_loader, criterion, device, epoch)\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f} | \"\n",
        "          f\"LR: {current_lr:.8f}\")\n",
        "\n",
        "    with open(metrics_csv, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([epoch+1, train_loss, val_loss, val_acc, current_lr])\n",
        "\n",
        "    # Save best model if val_loss decreased and val_acc increased\n",
        "    if val_loss < best_val_loss and val_acc > best_val_acc:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'label_encoder': label_encoder,\n",
        "        }, best_model_path)\n",
        "        print(f\"✅ Best model saved at epoch {epoch+1} with Val Loss {val_loss:.4f} and Val Acc {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if early_stopping.should_stop(val_loss):\n",
        "        print(\"🛑 Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Plot Train and Validation Loss\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.savefig(\"loss_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Validation Accuracy\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot Learning Rate over Epochs\n",
        "plt.plot(learning_rates, label=\"Learning Rate\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Rate Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.grid()\n",
        "plt.savefig(\"learning_rate_plot.png\")\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cqsXuvkoroj",
        "outputId": "97a65164-8423-48d3-d36d-aa9673951874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fine_tune_animal_clef.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Embeedings**"
      ],
      "metadata": {
        "id": "yL-c_ncVMF9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database"
      ],
      "metadata": {
        "id": "j3ji-xtIv4Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset ---\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.label_encoder[self.data.iloc[idx]['identity']]\n",
        "        return image, label\n",
        "\n",
        "# --- Transform (same as test) ---\n",
        "transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# --- Paths ---\n",
        "root = './animal-clef-2025'\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "# --- Label encoder ---\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# --- Datasets ---\n",
        "train_dataset = AnimalDataset(train_csv, root, label_encoder, transform)\n",
        "test_dataset = AnimalDataset(test_csv, root, label_encoder, transform)\n",
        "\n",
        "# --- Loaders ---\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load trained model ---\n",
        "# ConvNeXt model\n",
        "convnext = timm.create_model(\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "convnext_ckpt = torch.load(\"best_model_convnext.pth\", map_location=device)\n",
        "convnext.load_state_dict(convnext_ckpt['model_state_dict'])\n",
        "convnext = convnext.to(device)\n",
        "convnext.eval()\n",
        "\n",
        "# ConvNeXt model v2\n",
        "convnext_v2 = timm.create_model(\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "convnext_ckpt_v2 = torch.load(\"best_model_convnext_2.pth\", map_location=device)\n",
        "convnext_v2.load_state_dict(convnext_ckpt_v2['model_state_dict'])\n",
        "convnext_v2 = convnext_v2.to(device)\n",
        "convnext_v2.eval()\n",
        "\n",
        "# MaxViT model\n",
        "maxvit = timm.create_model(\n",
        "    'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "maxvit_ckpt = torch.load(\"best_model_maxvit.pth\", map_location=device)\n",
        "maxvit.load_state_dict(maxvit_ckpt['model_state_dict'])\n",
        "maxvit = maxvit.to(device)\n",
        "maxvit.eval()\n",
        "\n",
        "def extract_embeddings(dataloader, model, model_name=\"convnext\", split_name=\"train\"):\n",
        "    all_feats, all_labels = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"[{model_name}] Extracting {split_name} embeddings\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)  # Output: (B, C, H, W) or (B, D)\n",
        "            features = features.mean(dim=(-1, -2)) if features.dim() == 4 else features  # Global average pool if needed\n",
        "            all_feats.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    X = torch.cat(all_feats, dim=0).cpu().numpy()\n",
        "    y = torch.cat(all_labels, dim=0).cpu().numpy()\n",
        "\n",
        "    np.savez(f\"{split_name}_embeddings_{model_name}.npz\", X=X, y=y)\n",
        "    print(f\"✅ Saved {split_name}_embeddings_{model_name}.npz with shape {X.shape}\")\n",
        "\n",
        "# --- Run extraction ---\n",
        "# Extract embeddings with ConvNeXt\n",
        "extract_embeddings(train_loader, convnext, \"convnext\", \"train\")\n",
        "extract_embeddings(test_loader, convnext, \"convnext\", \"test\")\n",
        "\n",
        "# Extract embeddings with ConvNeXt v2\n",
        "extract_embeddings(train_loader, convnext_v2, \"convnext_v2\", \"train\")\n",
        "extract_embeddings(test_loader, convnext_v2, \"convnext_v2\", \"test\")\n",
        "\n",
        "# Extract embeddings with MaxViT\n",
        "extract_embeddings(train_loader, maxvit, \"maxvit\", \"train\")\n",
        "extract_embeddings(test_loader, maxvit, \"maxvit\", \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR8j-IqeMFdr",
        "outputId": "a63de341-fca0-4512-a2c7-d9d17202d263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query"
      ],
      "metadata": {
        "id": "px0-0vJRxEDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset fără identity ---\n",
        "class QueryDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.data.iloc[idx]['image_id']\n",
        "\n",
        "# --- Transform (same as test) ---\n",
        "transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# --- Paths ---\n",
        "root = './animal-clef-2025'\n",
        "query_csv = \"query_metadata.csv\"\n",
        "batch_size = 8\n",
        "\n",
        "# --- Query dataset ---\n",
        "query_dataset = QueryDataset(query_csv, root, transform)\n",
        "query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load label encoder from training set ---\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# --- Load trained models ---\n",
        "# ConvNeXt model\n",
        "convnext = timm.create_model(\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "convnext_ckpt = torch.load(\"best_model_convnext.pth\", map_location=device)\n",
        "convnext.load_state_dict(convnext_ckpt['model_state_dict'])\n",
        "convnext = convnext.to(device)\n",
        "convnext.eval()\n",
        "\n",
        "# ConvNeXt model v2\n",
        "convnext_v2 = timm.create_model(\n",
        "    'convnext_xlarge.fb_in22k_ft_in1k_384',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "convnext_ckpt_v2 = torch.load(\"best_model_convnext_2.pth\", map_location=device)\n",
        "convnext_v2.load_state_dict(convnext_ckpt_v2['model_state_dict'])\n",
        "convnext_v2 = convnext_v2.to(device)\n",
        "convnext_v2.eval()\n",
        "\n",
        "# MaxViT model\n",
        "maxvit = timm.create_model(\n",
        "    'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "maxvit_ckpt = torch.load(\"best_model_maxvit.pth\", map_location=device)\n",
        "maxvit.load_state_dict(maxvit_ckpt['model_state_dict'])\n",
        "maxvit = maxvit.to(device)\n",
        "maxvit.eval()\n",
        "\n",
        "# --- Embedding extractor fără etichete ---\n",
        "def extract_query_embeddings(dataloader, model, model_name=\"model\"):\n",
        "    all_feats = []\n",
        "    image_ids = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, ids in tqdm(dataloader, desc=f\"[{model_name}] Extracting query embeddings\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)\n",
        "            features = features.mean(dim=(-1, -2)) if features.dim() == 4 else features\n",
        "            all_feats.append(features.cpu())\n",
        "            image_ids.extend(ids)\n",
        "\n",
        "    X = torch.cat(all_feats, dim=0).numpy()\n",
        "    image_ids = np.array(image_ids)\n",
        "\n",
        "    np.savez(f\"query_embeddings_{model_name}.npz\", X=X, image_ids=image_ids)\n",
        "    print(f\"✅ Saved query_embeddings_{model_name}.npz with shape {X.shape}\")\n",
        "\n",
        "# --- Run extraction ---\n",
        "extract_query_embeddings(query_loader, convnext, \"convnext\")\n",
        "extract_query_embeddings(query_loader, convnext_v2, \"convnext_v2\")\n",
        "extract_query_embeddings(query_loader, maxvit, \"maxvit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-WO2Kq7xE2I",
        "outputId": "37827ba4-80c1-46fb-a338-21c4e6b1f79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database"
      ],
      "metadata": {
        "id": "jhxBLRaRv6Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset definition ---\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.label_encoder[self.data.iloc[idx]['identity']]\n",
        "        return image, label\n",
        "\n",
        "# --- Image transform ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# --- Paths and config ---\n",
        "root_dir = './animal-clef-2025'\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "batch_size = 8\n",
        "\n",
        "# --- Label encoder ---\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# --- Datasets and loaders ---\n",
        "train_dataset = AnimalDataset(train_csv, root_dir, label_encoder, transform)\n",
        "test_dataset = AnimalDataset(test_csv, root_dir, label_encoder, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load Swin transformer model ---\n",
        "def load_swin_model(ckpt_path, model_name):\n",
        "    model = timm.create_model(model_name, pretrained=False, num_classes=0)\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    filtered_state = {k: v for k, v in state_dict.items() if not k.startswith('head') and 'fc' not in k}\n",
        "    model.load_state_dict(filtered_state, strict=False)\n",
        "    return model.to(device).eval()\n",
        "\n",
        "swin = load_swin_model(\"best_model_swin_1.pth\", \"swin_large_patch4_window12_384\")\n",
        "swin_v2 = load_swin_model(\"best_model_swin_2.pth\", \"swin_large_patch4_window12_384\")\n",
        "\n",
        "def extract_embeddings(dataloader, model, model_name, split_name):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"[{model_name}] Extracting {split_name} embeddings (GAP)\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)\n",
        "\n",
        "            # Correct GAP: if output is (B, H, W, C), apply mean over dim 1 and 2\n",
        "            if features.dim() == 4:\n",
        "                if features.shape[1] == features.shape[2]:  # likely (B, H, W, C)\n",
        "                    features = features.mean(dim=(1, 2))  # --> (B, C)\n",
        "                else:  # fallback in case it's (B, C, H, W)\n",
        "                    features = features.mean(dim=(-1, -2))  # --> (B, C)\n",
        "\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    X = torch.cat(all_features, dim=0).numpy()\n",
        "    y = torch.cat(all_labels, dim=0).numpy()\n",
        "\n",
        "    np.savez(f\"{split_name}_embeddings_{model_name}.npz\", X=X, y=y)\n",
        "    print(f\"✅ Saved: {split_name}_embeddings_{model_name}.npz | Shape: {X.shape}\")\n",
        "\n",
        "# --- Run extraction ---\n",
        "extract_embeddings(train_loader, swin, \"swin\", \"train\")\n",
        "extract_embeddings(test_loader, swin, \"swin\", \"test\")\n",
        "\n",
        "extract_embeddings(train_loader, swin_v2, \"swin_v2\", \"train\")\n",
        "extract_embeddings(test_loader, swin_v2, \"swin_v2\", \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcvL9Y35U9g0",
        "outputId": "0dfd00e9-9461-40f1-a3d0-9941921de9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query"
      ],
      "metadata": {
        "id": "9r2h9r-gx7LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset fără identity --- (folosește image_id)\n",
        "class QueryDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.data.iloc[idx]['image_id']\n",
        "\n",
        "# --- Transform ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# --- Paths and config ---\n",
        "root_dir = './animal-clef-2025'\n",
        "query_csv = \"query_metadata.csv\"\n",
        "batch_size = 8\n",
        "\n",
        "# --- Query loader ---\n",
        "query_dataset = QueryDataset(query_csv, root_dir, transform)\n",
        "query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load Swin transformer model ---\n",
        "def load_swin_model(ckpt_path, model_name):\n",
        "    model = timm.create_model(model_name, pretrained=False, num_classes=0)\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    filtered_state = {k: v for k, v in state_dict.items() if not k.startswith('head') and 'fc' not in k}\n",
        "    model.load_state_dict(filtered_state, strict=False)\n",
        "    return model.to(device).eval()\n",
        "\n",
        "swin = load_swin_model(\"best_model_swin_1.pth\", \"swin_large_patch4_window12_384\")\n",
        "swin_v2 = load_swin_model(\"best_model_swin_2.pth\", \"swin_large_patch4_window12_384\")\n",
        "\n",
        "# --- Extract query embeddings ---\n",
        "def extract_query_embeddings(dataloader, model, model_name):\n",
        "    all_features = []\n",
        "    all_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, image_ids in tqdm(dataloader, desc=f\"[{model_name}] Extracting query embeddings (GAP)\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)\n",
        "\n",
        "            # Correct GAP: if output is (B, H, W, C), apply mean over dim 1 and 2\n",
        "            if features.dim() == 4:\n",
        "                if features.shape[1] == features.shape[2]:  # likely (B, H, W, C)\n",
        "                    features = features.mean(dim=(1, 2))  # --> (B, C)\n",
        "                else:  # fallback in case it's (B, C, H, W)\n",
        "                    features = features.mean(dim=(-1, -2))  # --> (B, C)\n",
        "\n",
        "            all_features.append(features.cpu())\n",
        "            all_ids.extend(image_ids)\n",
        "\n",
        "    X = torch.cat(all_features, dim=0).numpy()\n",
        "    ids = np.array(all_ids)\n",
        "\n",
        "    np.savez(f\"query_embeddings_{model_name}.npz\", X=X, image_ids=ids)\n",
        "    print(f\"✅ Saved: query_embeddings_{model_name}.npz | Shape: {X.shape}\")\n",
        "\n",
        "# --- Run extraction ---\n",
        "extract_query_embeddings(query_loader, swin, \"swin\")\n",
        "extract_query_embeddings(query_loader, swin_v2, \"swin_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m314Hu77x6Tt",
        "outputId": "2cdcfef1-e396-40fd-bbe5-d079007de42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database"
      ],
      "metadata": {
        "id": "z7zMKno9v8Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset Definition ---\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.label_encoder[self.data.iloc[idx]['identity']]\n",
        "        return image, label\n",
        "\n",
        "# --- Transform ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# --- Paths & Config ---\n",
        "root_dir = './animal-clef-2025'\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "batch_size = 8\n",
        "\n",
        "# --- Label Encoder ---\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# --- Data Loaders ---\n",
        "train_dataset = AnimalDataset(train_csv, root_dir, label_encoder, transform)\n",
        "test_dataset = AnimalDataset(test_csv, root_dir, label_encoder, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load MegaDescriptor Model ---\n",
        "def load_mega_descriptor():\n",
        "    model = timm.create_model(\n",
        "        'hf-hub:BVRA/MegaDescriptor-L-384',\n",
        "        pretrained=True,\n",
        "        num_classes=0\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_mega_descriptor()\n",
        "\n",
        "# --- Embedding Extraction ---\n",
        "def extract_embeddings(dataloader, model, model_name, split_name):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"[{model_name}] Extracting {split_name} embeddings\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)\n",
        "\n",
        "            # Apply Global Average Pooling if needed\n",
        "            if features.dim() == 4:\n",
        "                if features.shape[1] == features.shape[2]:\n",
        "                    features = features.mean(dim=(1, 2))  # (B, H, W, C) -> (B, C)\n",
        "                else:\n",
        "                    features = features.mean(dim=(-1, -2))  # (B, C, H, W) -> (B, C)\n",
        "\n",
        "            all_features.append(features.cpu())\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    X = torch.cat(all_features, dim=0).numpy()\n",
        "    y = torch.cat(all_labels, dim=0).numpy()\n",
        "    np.savez(f\"{split_name}_embeddings_{model_name}.npz\", X=X, y=y)\n",
        "    print(f\"✅ Saved: {split_name}_embeddings_{model_name}.npz | Shape: {X.shape}\")\n",
        "\n",
        "# --- Run Extraction ---\n",
        "extract_embeddings(train_loader, model, \"megadescriptor\", \"train\")\n",
        "extract_embeddings(test_loader, model, \"megadescriptor\", \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjUQm7WJgesw",
        "outputId": "5d0b452c-0240-4b6f-ee53-3239110f2953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query"
      ],
      "metadata": {
        "id": "U_llYW-VyQ4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extract_embeddings.py\n",
        "import torch\n",
        "import timm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Dataset pentru query fără etichete ---\n",
        "class QueryDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.data.iloc[idx]['image_id']\n",
        "\n",
        "# --- Transform ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((384, 384)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# --- Config ---\n",
        "root_dir = './animal-clef-2025'\n",
        "query_csv = \"query_metadata.csv\"\n",
        "batch_size = 8\n",
        "\n",
        "# --- Query loader ---\n",
        "query_dataset = QueryDataset(query_csv, root_dir, transform)\n",
        "query_loader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# --- Load MegaDescriptor ---\n",
        "def load_mega_descriptor():\n",
        "    model = timm.create_model(\n",
        "        'hf-hub:BVRA/MegaDescriptor-L-384',\n",
        "        pretrained=True,\n",
        "        num_classes=0\n",
        "    )\n",
        "    return model.to(device).eval()\n",
        "\n",
        "model = load_mega_descriptor()\n",
        "\n",
        "# --- Extract embeddings for query ---\n",
        "def extract_query_embeddings(dataloader, model, model_name=\"megadescriptor\"):\n",
        "    all_features = []\n",
        "    all_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, image_ids in tqdm(dataloader, desc=f\"[{model_name}] Extracting query embeddings\"):\n",
        "            images = images.to(device)\n",
        "            features = model.forward_features(images)\n",
        "\n",
        "            # Apply Global Average Pooling if needed\n",
        "            if features.dim() == 4:\n",
        "                if features.shape[1] == features.shape[2]:\n",
        "                    features = features.mean(dim=(1, 2))  # (B, H, W, C) -> (B, C)\n",
        "                else:\n",
        "                    features = features.mean(dim=(-1, -2))  # (B, C, H, W) -> (B, C)\n",
        "\n",
        "            all_features.append(features.cpu())\n",
        "            all_ids.extend(image_ids)\n",
        "\n",
        "    X = torch.cat(all_features, dim=0).numpy()\n",
        "    image_ids = np.array(all_ids)\n",
        "    np.savez(f\"query_embeddings_{model_name}.npz\", X=X, image_ids=image_ids)\n",
        "    print(f\"✅ Saved: query_embeddings_{model_name}.npz | Shape: {X.shape}\")\n",
        "\n",
        "# --- Run ---\n",
        "extract_query_embeddings(query_loader, model, \"megadescriptor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtAnjJjWyOhK",
        "outputId": "c44558d6-1e43-421b-d289-ff29ea960664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting extract_embeddings.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train a Classifier**"
      ],
      "metadata": {
        "id": "CqIhBIAhxIux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mlp_classifier.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Define label_encoder consistent with extract_embeddings.py ---\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "idx_to_label = {v: k for k, v in label_encoder.items()}\n",
        "\n",
        "# --- Load all embeddings ---\n",
        "def load_embeddings(keys, prefix=\"train\"):\n",
        "    return {\n",
        "        key: np.load(f\"{prefix}_embeddings_{key}.npz\") for key in keys\n",
        "    }\n",
        "\n",
        "def pool_if_needed(X):\n",
        "    return X.mean(axis=(2, 3)) if X.ndim == 4 else X\n",
        "\n",
        "# --- Scheduler helper ---\n",
        "def linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# --- Keys and data loading ---\n",
        "keys = [\"convnext\", \"convnext_v2\", \"maxvit\", \"swin\", \"swin_v2\", \"megadescriptor\"]\n",
        "train_embeddings = load_embeddings(keys, \"train\")\n",
        "test_embeddings = load_embeddings(keys, \"test\")\n",
        "\n",
        "# --- Preprocess ---\n",
        "X_train_list, X_test_list = [], []\n",
        "for k in keys:\n",
        "    print(f\"📦 Using {k}\")\n",
        "    X_train_list.append(pool_if_needed(train_embeddings[k][\"X\"]))\n",
        "    X_test_list.append(pool_if_needed(test_embeddings[k][\"X\"]))\n",
        "\n",
        "X_train = np.concatenate(X_train_list, axis=1)\n",
        "X_test = np.concatenate(X_test_list, axis=1)\n",
        "y_train = train_embeddings[keys[0]][\"y\"]\n",
        "y_test = test_embeddings[keys[0]][\"y\"]\n",
        "\n",
        "# --- Normalize ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, \"mlp_scaler.joblib\")\n",
        "\n",
        "# --- PCA ---\n",
        "pca = PCA(n_components=0.99, svd_solver='full')\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "joblib.dump(pca, \"mlp_pca.joblib\")\n",
        "\n",
        "print(f\"🔢 Final input dimension after PCA: {X_train_pca.shape[1]}\")\n",
        "\n",
        "# --- Torch datasets ---\n",
        "X_train_tensor = torch.tensor(X_train_pca, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "\n",
        "# --- Model ---\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP(X_train_pca.shape[1], len(np.unique(y_train))).cuda()\n",
        "\n",
        "# --- Training setup ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)\n",
        "\n",
        "num_epochs = 100\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "scheduler = linear_schedule_with_warmup(optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps)\n",
        "\n",
        "best_loss = float('inf')\n",
        "early_stop_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "print(\"\\n🚀 Training MLP model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item()\n",
        "    avg_loss_train = running_loss / len(train_loader)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    running_loss_test = 0.0\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test_tensor.cuda())\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "            out = model(X_batch)\n",
        "            loss = criterion(out, y_batch)\n",
        "            running_loss_test += loss.item()\n",
        "        avg_loss_test = running_loss_test / len(test_loader)\n",
        "\n",
        "        train_losses.append(avg_loss_train)\n",
        "        test_losses.append(avg_loss_test)\n",
        "        val_accuracies.append(acc)\n",
        "\n",
        "        print(f\"📉 Train Loss: {avg_loss_train:.4f} | 🧪 Test Loss: {avg_loss_test:.4f} | 🎯 Val Accuracy: {acc:.4f}\")\n",
        "\n",
        "    if avg_loss_test < best_loss:\n",
        "        best_loss = avg_loss_test\n",
        "        torch.save(model.state_dict(), \"mlp_classifier_best.pt\")\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= 10:\n",
        "            print(\"🛑 Early stopping triggered (based on test loss).\")\n",
        "            break\n",
        "\n",
        "# --- Plot and save training/test loss ---\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Test Loss')\n",
        "plt.legend()\n",
        "plt.savefig(\"loss_plot.png\")\n",
        "print(\"📊 Saved loss plot to loss_plot.png\")\n",
        "\n",
        "# --- Plot and save validation accuracy ---\n",
        "plt.figure()\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.savefig(\"accuracy_plot.png\")\n",
        "print(\"📈 Saved accuracy plot to accuracy_plot.png\")\n",
        "\n",
        "# --- Final evaluation ---\n",
        "print(\"\\n🔍 Evaluating best model...\")\n",
        "model.load_state_dict(torch.load(\"mlp_classifier_best.pt\"))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor.cuda())\n",
        "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"\\n✅ Accuracy: {acc:.4f}\")\n",
        "    print(\"\\n📊 Classification Report:\")\n",
        "    print(classification_report(y_test, preds, digits=4))\n",
        "\n",
        "# --- Save classification report to CSV ---\n",
        "report_dict = classification_report(y_test, preds, output_dict=True)\n",
        "df_report = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "df_report = df_report[df_report.index.to_series().str.isdigit()]\n",
        "df_report[\"label_index\"] = df_report.index.astype(int)\n",
        "df_report[\"identity\"] = df_report[\"label_index\"].map(idx_to_label)\n",
        "df_report[\"underrepresented\"] = df_report[\"support\"] < 5\n",
        "df_report[\"not_predicted\"] = df_report[\"recall\"] == 0.0\n",
        "\n",
        "df_report = df_report[[\n",
        "    \"identity\", \"label_index\", \"support\", \"precision\", \"recall\", \"f1-score\",\n",
        "    \"underrepresented\", \"not_predicted\"\n",
        "]]\n",
        "\n",
        "df_report.to_csv(\"class_wise_metrics.csv\", index=False)\n",
        "print(\"📁 Saved class-wise classification report to class_wise_metrics.csv\")\n",
        "\n",
        "# --- Query inference ---\n",
        "print(\"\\n🔍 Running inference on query set...\")\n",
        "\n",
        "# Load query embeddings\n",
        "query_embeddings = []\n",
        "query_ids = []\n",
        "\n",
        "for k in keys:\n",
        "    data = np.load(f\"query_embeddings_{k}.npz\")\n",
        "    X_query_k = pool_if_needed(data[\"X\"])\n",
        "    query_embeddings.append(X_query_k)\n",
        "    if len(query_ids) == 0:\n",
        "        query_ids = data[\"image_ids\"]\n",
        "\n",
        "X_query = np.concatenate(query_embeddings, axis=1)\n",
        "\n",
        "# Apply scaler and PCA\n",
        "scaler = joblib.load(\"mlp_scaler.joblib\")\n",
        "pca = joblib.load(\"mlp_pca.joblib\")\n",
        "\n",
        "X_query_scaled = scaler.transform(X_query)\n",
        "X_query_pca = pca.transform(X_query_scaled)\n",
        "\n",
        "# Predict with MLP\n",
        "X_query_tensor = torch.tensor(X_query_pca, dtype=torch.float32).cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_query_tensor)\n",
        "    probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "# Map predicted class indices back to labels\n",
        "pred_labels = [idx_to_label[p] for p in preds]\n",
        "confidences = probs.max(axis=1)\n",
        "\n",
        "# Save predictions\n",
        "df_out = pd.DataFrame({\n",
        "    \"image_id\": query_ids,\n",
        "    \"identity\": pred_labels,\n",
        "    \"confidence\": confidences\n",
        "})\n",
        "df_out.to_csv(\"query_predictions.csv\", index=False)\n",
        "print(f\"📁 Saved query predictions to query_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmdItbUSg_cA",
        "outputId": "0a70c8b5-0e91-4224-b6ed-fc26e870da41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mlp_classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mega-Descriptor**"
      ],
      "metadata": {
        "id": "0IiN-rqukMOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_mega.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torchvision.transforms as T\n",
        "from wildlife_tools.features import DeepFeatures\n",
        "from wildlife_tools.similarity import CosineSimilarity\n",
        "from wildlife_tools.similarity.wildfusion import SimilarityPipeline, WildFusion\n",
        "from wildlife_tools.similarity.pairwise.lightglue import MatchLightGlue\n",
        "from wildlife_tools.features.local import AlikedExtractor\n",
        "from wildlife_tools.similarity.calibration import IsotonicCalibration\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "class InferenceDataset(Dataset):\n",
        "    def __init__(self, df, root_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.metadata = self.df\n",
        "        self.col_label = \"identity\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.df.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, idx\n",
        "\n",
        "# Paths\n",
        "root = './animal-clef-2025'\n",
        "\n",
        "# Transforms\n",
        "transform_display = T.Compose([T.Resize([384, 384])])\n",
        "transform = T.Compose([\n",
        "    *transform_display.transforms,\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "transforms_aliked = T.Compose([\n",
        "    T.Resize([512, 512]),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "# Load metadata\n",
        "df_database = pd.read_csv(\"database_metadata.csv\")\n",
        "df_query = pd.read_csv(\"query_metadata.csv\")\n",
        "df_calibration = pd.read_csv(\"test_database_metadata.csv\")\n",
        "\n",
        "n_query = len(df_query)\n",
        "\n",
        "# Datasets\n",
        "calib_ds = InferenceDataset(df_calibration, root, transform)\n",
        "query_ds = InferenceDataset(df_query, root, transform)\n",
        "database_ds = InferenceDataset(df_database, root, transform)\n",
        "\n",
        "# Load MegaDescriptor model\n",
        "print(\"🔄 Loading MegaDescriptor model...\")\n",
        "model = timm.create_model('hf-hub:BVRA/MegaDescriptor-L-384', num_classes=0, pretrained=True)\n",
        "device = 'cuda'\n",
        "\n",
        "# Define similarity pipelines\n",
        "matcher_aliked = SimilarityPipeline(\n",
        "    matcher=MatchLightGlue(features='aliked', device=device, batch_size=16),\n",
        "    extractor=AlikedExtractor(),\n",
        "    transform=transforms_aliked,\n",
        "    calibration=IsotonicCalibration()\n",
        ")\n",
        "\n",
        "matcher_mega = SimilarityPipeline(\n",
        "    matcher=CosineSimilarity(),\n",
        "    extractor=DeepFeatures(model=model, device=device, batch_size=16),\n",
        "    transform=transform,\n",
        "    calibration=IsotonicCalibration()\n",
        ")\n",
        "\n",
        "# Calibrate WildFusion with progress bar\n",
        "print(\"🧪 Calibrating WildFusion...\")\n",
        "wildfusion = WildFusion(\n",
        "    calibrated_pipelines=[matcher_aliked, matcher_mega],\n",
        "    priority_pipeline=matcher_mega\n",
        ")\n",
        "\n",
        "with tqdm(total=1, desc=\"Fitting Calibration\") as pbar:\n",
        "    wildfusion.fit_calibration(calib_ds, calib_ds)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Compute similarities\n",
        "print(\"🔍 Computing similarities using WildFusion...\")\n",
        "similarity = wildfusion(query_ds, database_ds, B=None)\n",
        "\n",
        "# Get predictions\n",
        "pred_idx = similarity.argmax(axis=1)\n",
        "pred_scores = similarity[np.arange(n_query), pred_idx]\n",
        "identities = df_database[\"identity\"].tolist()\n",
        "predictions = [identities[i] for i in pred_idx]\n",
        "image_ids = df_query[\"image_id\"].tolist()\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    \"image_id\": image_ids,\n",
        "    \"identity\": predictions,\n",
        "    \"confidence\": pred_scores\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ submission.csv has been created successfully.\")\n",
        "\n",
        "# Save the model and WildFusion object\n",
        "save_dict = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'wildfusion': wildfusion,\n",
        "    'df_database': df_database,\n",
        "    'df_query': df_query,\n",
        "    'df_calibration': df_calibration\n",
        "}\n",
        "\n",
        "torch.save(save_dict, \"wildfusion_model.pth\")\n",
        "print(\"💾 Model saved to 'wildfusion_model.pth'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfYaxeAykOF6",
        "outputId": "dcf87d69-30fd-4eb6-c58e-4228d642ace6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_mega.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Soup**"
      ],
      "metadata": {
        "id": "YGPavhyEDKu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_soup.py\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_scheduler\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "        return image, label\n",
        "\n",
        "# Early stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0005):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "# Train and validation functions\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Training (Epoch {epoch+1})\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device, epoch=0):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Validating (Epoch {epoch+1})\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n",
        "\n",
        "# Load model state dict\n",
        "def load_state_dict(path):\n",
        "    checkpoint = torch.load(path, map_location='cpu')\n",
        "    return checkpoint['model_state_dict']\n",
        "\n",
        "# Average multiple model weights\n",
        "def weighted_average_models(model_paths, weights, model_arch, num_classes, device):\n",
        "    assert len(model_paths) == len(weights), \"Numărul de modele și de ponderi trebuie să coincidă.\"\n",
        "    weights = [float(w) for w in weights]\n",
        "    total_weight = sum(weights)\n",
        "    norm_weights = [w / total_weight for w in weights]\n",
        "\n",
        "    base_model = timm.create_model(model_arch, pretrained=False, num_classes=num_classes).to(device)\n",
        "    avg_state_dict = None\n",
        "\n",
        "    for i, path in enumerate(model_paths):\n",
        "        state_dict = load_state_dict(path)\n",
        "        weight = norm_weights[i]\n",
        "        if avg_state_dict is None:\n",
        "            avg_state_dict = {k: v.clone().float() * weight for k, v in state_dict.items()}\n",
        "        else:\n",
        "            for k in avg_state_dict:\n",
        "                avg_state_dict[k] += state_dict[k].float() * weight\n",
        "\n",
        "    base_model.load_state_dict(avg_state_dict)\n",
        "    return base_model\n",
        "\n",
        "# Paths\n",
        "root = './animal-clef-2025'\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "# Transforms\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    T.RandomErasing(p=0.25)\n",
        "])\n",
        "test_transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Label encoder\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 8\n",
        "train_dataset = AnimalDataset(train_csv, root, label_encoder, train_transform)\n",
        "test_dataset = AnimalDataset(test_csv, root, label_encoder, test_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Train a single model (optional training loop - can skip if you're only testing soup)\n",
        "model = timm.create_model('convnext_xlarge.fb_in22k_ft_in1k_384', pretrained=True, num_classes=len(label_encoder)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=8e-5, weight_decay=0.01)\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=int(0.1 * len(train_loader) * 30), num_training_steps=len(train_loader) * 30)\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate Weighted Model Soup\n",
        "# -------------------------------\n",
        "model_paths = [\"best_model_1.pth\", \"best_model_2.pth\", \"best_model_3.pth\"]\n",
        "weights = [0.7, 0.15, 0.15]\n",
        "\n",
        "model_soup = weighted_average_models(model_paths, weights, 'convnext_xlarge.fb_in22k_ft_in1k_384', len(label_encoder), device)\n",
        "model_soup.eval()\n",
        "\n",
        "# Save the weighted soup model\n",
        "soup_model_path = \"weighted_model_soup.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model_soup.state_dict(),\n",
        "    'label_encoder': label_encoder\n",
        "}, soup_model_path)\n",
        "print(f\"💾 Weighted Model Soup saved to {soup_model_path}\")\n",
        "\n",
        "test_loss, test_acc = validate(model_soup, test_loader, criterion, device)\n",
        "print(f\"📊 Weighted Model Soup Evaluation -> Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "O6OpcgRaDMQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fce7c1d-8262-4d5f-a295-586564097957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_soup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Soft Ensemble Evaluation**"
      ],
      "metadata": {
        "id": "Z4wbQawJHiBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_soup.py\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_scheduler\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class\n",
        "class AnimalDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, label_encoder, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.data.iloc[idx]['path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label_str = self.data.iloc[idx]['identity']\n",
        "        label = self.label_encoder[label_str]\n",
        "        return image, label\n",
        "\n",
        "# Early stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0005):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        return self.counter >= self.patience\n",
        "\n",
        "# Train and validation functions\n",
        "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(dataloader, desc=f\"Training (Epoch {epoch+1})\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion, device, epoch=0):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=f\"Validating (Epoch {epoch+1})\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return running_loss / len(dataloader), accuracy\n",
        "\n",
        "# Load model state dict\n",
        "def load_state_dict(path):\n",
        "    checkpoint = torch.load(path, map_location='cpu')\n",
        "    return checkpoint['model_state_dict']\n",
        "\n",
        "def evaluate_soft_ensemble(model1, model2, dataloader, criterion, device, alpha=0.5):\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"🧪 Evaluating Soft Ensemble\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Predict logits\n",
        "            logits1 = model1(images)\n",
        "            logits2 = model2(images)\n",
        "\n",
        "            # Convert to softmax probabilities\n",
        "            probs1 = F.softmax(logits1, dim=1)\n",
        "            probs2 = F.softmax(logits2, dim=1)\n",
        "\n",
        "            # Weighted average\n",
        "            blended_probs = alpha * probs1 + (1 - alpha) * probs2\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(blended_probs.log(), labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Final prediction\n",
        "            preds = blended_probs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Paths\n",
        "root = './animal-clef-2025'\n",
        "train_csv = \"train_database_metadata.csv\"\n",
        "test_csv = \"test_database_metadata.csv\"\n",
        "\n",
        "# Transforms\n",
        "train_transform = T.Compose([\n",
        "    T.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "    T.AutoAugment(policy=T.AutoAugmentPolicy.IMAGENET),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    T.RandomErasing(p=0.25)\n",
        "])\n",
        "test_transform = T.Compose([\n",
        "    T.Resize([384, 384]),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Label encoder\n",
        "train_identities = pd.read_csv(train_csv)['identity'].unique()\n",
        "label_encoder = {identity: idx for idx, identity in enumerate(sorted(train_identities))}\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 8\n",
        "train_dataset = AnimalDataset(train_csv, root, label_encoder, train_transform)\n",
        "test_dataset = AnimalDataset(test_csv, root, label_encoder, test_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load both models\n",
        "model1 = timm.create_model('convnext_xlarge.fb_in22k_ft_in1k_384', pretrained=False, num_classes=len(label_encoder)).to(device)\n",
        "model1.load_state_dict(torch.load(\"best_model_convnext.pth\", map_location=device)['model_state_dict'])\n",
        "\n",
        "model2 = timm.create_model('maxvit_xlarge_tf_384.in21k_ft_in1k', pretrained=False, num_classes=len(label_encoder)).to(device)\n",
        "model2.load_state_dict(torch.load(\"best_model_maxvit.pth\", map_location=device)['model_state_dict'])\n",
        "\n",
        "# Evaluate\n",
        "alpha = 0.6  # weight for model1\n",
        "soft_ensemble_loss, soft_ensemble_acc = evaluate_soft_ensemble(model1, model2, test_loader, criterion, device, alpha)\n",
        "print(f\"📊 Soft Ensemble (α={alpha}) -> Test Loss: {soft_ensemble_loss:.4f} | Test Accuracy: {soft_ensemble_acc:.4f}\")"
      ],
      "metadata": {
        "id": "-XSO5F-iH7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "sTgFOpoHcj6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference_query.py\n",
        "import torch\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"best_model.pth\"\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "label_encoder = checkpoint['label_encoder']\n",
        "idx_to_label = {v: k for k, v in label_encoder.items()}\n",
        "\n",
        "model = timm.create_model(\n",
        "    'maxvit_xlarge_tf_384.in21k_ft_in1k',\n",
        "    pretrained=False,\n",
        "    num_classes=len(label_encoder)\n",
        ")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Prediction function without threshold filtering\n",
        "def predict(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        conf, preds = probs.max(dim=1)\n",
        "        confidence = conf.item()\n",
        "        predicted_idx = preds.item()\n",
        "\n",
        "    predicted_label = idx_to_label[predicted_idx]\n",
        "    return predicted_label, confidence\n",
        "\n",
        "# Load query data\n",
        "query_csv = \"query_metadata.csv\"\n",
        "query_data = pd.read_csv(query_csv)\n",
        "\n",
        "root = \"./animal-clef-2025\"\n",
        "predictions = []\n",
        "confidences = []\n",
        "\n",
        "# Inference loop\n",
        "for idx, row in tqdm(query_data.iterrows(), total=len(query_data), desc=\"Predicting\"):\n",
        "    img_rel_path = row['path']\n",
        "    img_full_path = os.path.join(root, img_rel_path)\n",
        "\n",
        "    try:\n",
        "        pred_label, conf = predict(img_full_path)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error with image {img_full_path}: {e}\")\n",
        "        pred_label, conf = \"error\", 0.0\n",
        "\n",
        "    predictions.append(pred_label)\n",
        "    confidences.append(conf)\n",
        "\n",
        "# Save results\n",
        "query_data['identity'] = predictions\n",
        "query_data['confidence'] = confidences\n",
        "\n",
        "output_csv = \"query_with_predictions.csv\"\n",
        "query_data.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"✅ File saved at {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE4zFPrCfNRd",
        "outputId": "a5667371-b020-4ddb-9b21-070de6b81491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing inference_query.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Fusion Strategy**"
      ],
      "metadata": {
        "id": "IxlObTO_OP-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two prediction files\n",
        "df1 = pd.read_csv(\"preds_1_mega_descriptor.csv\")\n",
        "df2 = pd.read_csv(\"preds_4_mega_descriptor.csv\")\n",
        "\n",
        "# Rename columns to avoid name conflicts\n",
        "df1 = df1.rename(columns={\"identity\": \"identity_1\", \"confidence\": \"confidence_1\"})\n",
        "df2 = df2.rename(columns={\"identity\": \"identity_2\", \"confidence\": \"confidence_2\"})\n",
        "\n",
        "# Merge the dataframes on 'image_id'\n",
        "merged = df1.merge(df2, on=\"image_id\")\n",
        "\n",
        "# Prepare the final results\n",
        "result_rows = []\n",
        "\n",
        "for _, row in merged.iterrows():\n",
        "    image_id = row[\"image_id\"]\n",
        "\n",
        "    identity_1 = row[\"identity_1\"]\n",
        "    identity_2 = row[\"identity_2\"]\n",
        "\n",
        "    # Weighted confidence: slightly favor the second model\n",
        "    confidence_1 = row[\"confidence_1\"] * 0.45\n",
        "    confidence_2 = row[\"confidence_2\"] * 0.55\n",
        "\n",
        "    if identity_1 == identity_2:\n",
        "        final_identity = identity_1\n",
        "        final_confidence = (confidence_1 + confidence_2) * 1.2\n",
        "    else:\n",
        "        final_identity = identity_2\n",
        "        final_confidence = confidence_2 * 1.82\n",
        "\n",
        "    result_rows.append({\n",
        "        \"image_id\": image_id,\n",
        "        \"identity\": final_identity,\n",
        "        \"confidence\": final_confidence\n",
        "    })\n",
        "\n",
        "# Create and save the final DataFrame\n",
        "result_df = pd.DataFrame(result_rows)\n",
        "result_df.to_csv(\"preds_mega_descriptor.csv\", index=False)\n",
        "\n",
        "print(\"✅ File 'preds_mega_descriptor.csv' has been created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeoqdDb2v6eL",
        "outputId": "12cea717-42fe-40d8-829c-212d0b7c13cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File 'preds_mega_descriptor.csv' has been created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load predictions from the two models\n",
        "df_mega = pd.read_csv(\"preds_mega_descriptor.csv\")\n",
        "df_conv1 = pd.read_csv(\"preds_convnext_1.csv\")\n",
        "\n",
        "# Rename columns to avoid conflicts\n",
        "df_mega = df_mega.rename(columns={\"identity\": \"identity_mega\", \"confidence\": \"confidence_mega\"})\n",
        "df_conv1 = df_conv1.rename(columns={\"identity\": \"identity_conv1\", \"confidence\": \"confidence_conv1\"})\n",
        "\n",
        "# Merge predictions on image_id\n",
        "merged = df_mega.merge(df_conv1, on=\"image_id\")\n",
        "\n",
        "# Prepare final results\n",
        "result_rows = []\n",
        "\n",
        "for _, row in merged.iterrows():\n",
        "    image_id = row[\"image_id\"]\n",
        "\n",
        "    identity_mega = row[\"identity_mega\"]\n",
        "    identity_conv1 = row[\"identity_conv1\"]\n",
        "\n",
        "    confidence_mega = row[\"confidence_mega\"]\n",
        "    confidence_conv1 = row[\"confidence_conv1\"] * 0.25\n",
        "\n",
        "    if identity_mega == identity_conv1:\n",
        "        final_identity = identity_mega\n",
        "        final_confidence = ((confidence_mega + confidence_conv1) / 2) * 1.7\n",
        "    else:\n",
        "        if confidence_mega >= confidence_conv1:\n",
        "            final_identity = identity_mega\n",
        "            final_confidence = confidence_mega\n",
        "        else:\n",
        "            final_identity = identity_conv1\n",
        "            final_confidence = confidence_conv1 * 1.2\n",
        "\n",
        "    result_rows.append({\n",
        "        \"image_id\": image_id,\n",
        "        \"identity\": final_identity,\n",
        "        \"confidence\": final_confidence\n",
        "    })\n",
        "\n",
        "# Create and save final DataFrame\n",
        "result_df = pd.DataFrame(result_rows)\n",
        "result_df.to_csv(\"final_submission.csv\", index=False)\n",
        "\n",
        "print(\"✅ The file 'final_submission.csv' has been created using df_mega and df_conv1.\")\n"
      ],
      "metadata": {
        "id": "u2L8BoR_VOuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4204e8d6-84d8-47bf-876c-79602e8165c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ The file 'final_submission.csv' has been created using df_mega and df_conv1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(\"final_submission.csv\")\n",
        "\n",
        "# Scale confidence values to percentage\n",
        "df[\"confidence_percent\"] = df[\"confidence\"] * 100\n",
        "\n",
        "# Define bins and labels\n",
        "bins = [30, 35, 40, 45, 50, 52, 54, 55, 60, 65, 70, 75, 80, 85]\n",
        "labels = [\"30–35\", \"35–40\", \"40–45\", \"45–50\", \"50-52\", \"52-54\", \"54-55\", \"55–60\", \"60–65\", \"65–70\", \"70–75\", \"75–80\", \"80–85\"]\n",
        "\n",
        "# Bin the data\n",
        "df[\"interval\"] = pd.cut(df[\"confidence_percent\"], bins=bins, labels=labels, right=False)\n",
        "\n",
        "# Count occurrences per interval\n",
        "counts = df[\"interval\"].value_counts().sort_index()\n",
        "\n",
        "print(\"Confidence (%) value distribution per interval:\")\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucGPmBetVGsi",
        "outputId": "a3e98709-6a0b-4a5b-b0b8-7b11eb5e8a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence (%) value distribution per interval:\n",
            "interval\n",
            "30–35    146\n",
            "35–40    178\n",
            "40–45    289\n",
            "45–50     82\n",
            "50-52     41\n",
            "52-54     21\n",
            "54-55     15\n",
            "55–60     35\n",
            "60–65     60\n",
            "65–70     77\n",
            "70–75     45\n",
            "75–80     36\n",
            "80–85     35\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"final_submission.csv\")\n",
        "\n",
        "# Apply threshold: if confidence < 0.6, set identity to 'new_individual'\n",
        "df.loc[df[\"confidence\"] < 0.45, \"identity\"] = \"new_individual\"\n",
        "\n",
        "# Save the updated DataFrame back to CSV\n",
        "df.to_csv(\"final_submission_thresholded.csv\", index=False)\n",
        "\n",
        "print(\"The file 'final_submission_thresholded.csv' has been created with the threshold applied.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKXRpxPLTGk8",
        "outputId": "55eed1a4-85b0-41a9-bc61-301df4b805d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file 'final_submission_thresholded.csv' has been created with the threshold applied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample Submission**"
      ],
      "metadata": {
        "id": "0Xi7kMRKpgHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "query_predictions = pd.read_csv('final_submission_thresholded.csv')\n",
        "\n",
        "submission = query_predictions[['image_id', 'identity']]\n",
        "\n",
        "submission.to_csv('sample_submission.csv', index=False)\n",
        "\n",
        "print(\"✅ sample_submission.csv done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB-nWwj6piT3",
        "outputId": "4df5fc1d-d511-4d20-bc94-f94a1ce9ed86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ sample_submission.csv done!\n"
          ]
        }
      ]
    }
  ]
}